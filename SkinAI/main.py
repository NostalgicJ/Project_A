import os
import sys
import json
import uuid
import sqlite3
import csv
import torch
import numpy as np
import subprocess # OCR ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ìš©
import shutil     # í´ë” ì²­ì†Œìš©
from datetime import datetime
from flask import Flask, send_from_directory, request, jsonify
from flask_cors import CORS
from unittest.mock import MagicMock
from itertools import combinations

# ------------------------------------------------------
# [1] Matplotlib ì—ëŸ¬ ë°©ì§€ (Mocking)
# ------------------------------------------------------
sys.modules["matplotlib"] = MagicMock()
sys.modules["matplotlib.pyplot"] = MagicMock()

# ------------------------------------------------------
# [2] ê²½ë¡œ ì„¤ì •
# ------------------------------------------------------
BASE_DIR = os.path.abspath(os.path.dirname(__file__))

# server í´ë” ìœ„ì¹˜ ì°¾ê¸°
if os.path.exists(os.path.join(BASE_DIR, 'server')):
    SERVER_ROOT = os.path.join(BASE_DIR, 'server')
else:
    SERVER_ROOT = os.path.abspath(os.path.join(BASE_DIR, '..', 'server'))

SHARED_DIR = os.path.join(SERVER_ROOT, 'src', 'shared')
sys.path.append(SHARED_DIR)

MODEL_PATH = os.path.join(SERVER_ROOT, 'storage', 'models', 'trained', 'gnn_final_20251111', 'gnn_model_final.pth')
CSV_PATH = os.path.join(BASE_DIR, 'processed_cosmetics_final_2.csv')
DB_PATH = os.path.join(BASE_DIR, 'skinai.db')

# OCR ê²½ë¡œ
OCR_ROOT = os.path.join(BASE_DIR, 'ocr_model') 
OCR_INPUT_DIR = os.path.join(OCR_ROOT, 'CRAFT_Make_Polygon', 'my_test_images')
OCR_RESULT_DIR = os.path.join(OCR_ROOT, 'inference', 'inference_result')

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def find_model_path(root_dir):
    search_start = os.path.join(root_dir, 'storage')
    if not os.path.exists(search_start): return None
    for root, dirs, files in os.walk(search_start):
        for file in files:
            if file.endswith('.pth') and 'gnn' in file:
                return os.path.join(root, file)
    return None

# ------------------------------------------------------
# [3] Flask ì•± ì„¤ì •
# ------------------------------------------------------
app = Flask(__name__,
            static_url_path='/',
            static_folder=os.path.join(BASE_DIR, 'dist', 'public'),
            template_folder=os.path.join(BASE_DIR, 'dist', 'public'))
CORS(app)

# ------------------------------------------------------
# [4] CSV ë¡œë“œ
# ------------------------------------------------------
PRODUCT_DICT = {} 
def load_products_from_csv():
    if not os.path.exists(CSV_PATH):
        print(f"âŒ [ì˜¤ë¥˜] CSV íŒŒì¼ ì—†ìŒ: {CSV_PATH}")
        return
    try:
        print(f"ğŸ“‚ CSV ë¡œë”© ì¤‘: {CSV_PATH}")
        with open(CSV_PATH, mode='r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            count = 0
            for row in reader:
                name = row.get('ì œí’ˆëª…_ì •ë¦¬')
                ingredients_str = row.get('ì„±ë¶„_ë¬¸ìì—´')
                if name and ingredients_str:
                    PRODUCT_DICT[name] = ingredients_str.split(' ')
                    count += 1
            print(f"âœ… CSV ë¡œë“œ ì™„ë£Œ! ({count}ê°œ)")
    except Exception as e:
        print(f"âŒ CSV ì—ëŸ¬: {e}")

load_products_from_csv()

# ------------------------------------------------------
# [5] DB ì´ˆê¸°í™”
# ------------------------------------------------------
def init_history_db():
    conn = sqlite3.connect(DB_PATH)
    conn.execute('''CREATE TABLE IF NOT EXISTS user_products (id TEXT PRIMARY KEY, name TEXT, ingredients TEXT, date TEXT)''')
    conn.execute('''CREATE TABLE IF NOT EXISTS analysis_history (id TEXT PRIMARY KEY, date TEXT, items TEXT, result TEXT)''')
    conn.commit()
    conn.close()

init_history_db()

# ------------------------------------------------------
# [6] GNN ëª¨ë¸ ë¡œë“œ
# ------------------------------------------------------
gnn_analyzer = None
collate_fn = None 
IngredientFormulaDataset = None 

def load_gnn_model():
    global gnn_analyzer, collate_fn, IngredientFormulaDataset
    try:
        from gnn_final_20251111 import GNNCosmeticAnalyzer, GNNCollate, IngredientFormulaDataset as IFD
        collate_fn = GNNCollate()
        IngredientFormulaDataset = IFD
        gnn_analyzer = GNNCosmeticAnalyzer()
        
        real_model_path = find_model_path(SERVER_ROOT)
        if real_model_path:
            gnn_analyzer.load_model(real_model_path)
            print(f"ğŸš€ GNN ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        else:
            print(f"âš ï¸ ëª¨ë¸ íŒŒì¼ ì—†ìŒ")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

load_gnn_model()

# â˜…â˜…â˜… [í•µì‹¬] GNN ì¶”ë¡  ë¡œì§ (1:1 ì¡°í•© ë¶„ì„ + ì ìˆ˜ ë³´ì •) â˜…â˜…â˜…
def run_gnn_inference(ingredients_list):
    if not gnn_analyzer: return {"score": 0, "status": "UNKNOWN", "message": "ëª¨ë¸ ë¯¸ë¡œë“œ"}
    
    try:
        # 1. ì„±ë¶„ í•„í„°ë§ (ëª¨ë¸ì´ ì•„ëŠ” ì„±ë¶„ë§Œ)
        vocab = gnn_analyzer.vocab_to_idx
        valid_ingredients = [ing for ing in ingredients_list if vocab.get(ing, 0) != 0]
        
        print(f"ğŸ” [ë¶„ì„ ì‹œì‘] ì…ë ¥: {len(ingredients_list)}ê°œ -> ìœ íš¨: {len(valid_ingredients)}ê°œ")
        print(f"   (ì¸ì‹ëœ ì„±ë¶„ ì˜ˆì‹œ: {valid_ingredients[:5]}...)")
        
        if len(valid_ingredients) < 2:
            return {
                "score": 100, 
                "status": "SAFE", 
                "message": "ë¶„ì„í•  ì„±ë¶„ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (OCR ì¸ì‹ ì‹¤íŒ¨ ë˜ëŠ” ì„±ë¶„ ë¶€ì¡±)",
                "problematic_ingredients": []
            }

        # 2. ëª¨ë“  2ê°œ ì¡°í•© ìƒì„± (Pairs)
        pairs = list(combinations(valid_ingredients, 2))
        if len(pairs) > 3000: 
            import random
            pairs = random.sample(pairs, 3000)

        # 3. ë°°ì¹˜ ì¶”ë¡ 
        formulas = [(list(pair), 0.0, 0.0, False) for pair in pairs]
        dataset = IngredientFormulaDataset(formulas, vocab)
        loader = torch.utils.data.DataLoader(dataset, batch_size=64, collate_fn=collate_fn)
        
        gnn_analyzer.model.eval()
        max_danger = 0.0
        worst_pair = [] 
        
        with torch.no_grad():
            batch_idx = 0
            for batch in loader:
                if hasattr(batch, 'to'): batch = batch.to(DEVICE)
                outputs = gnn_analyzer.model(batch)
                dangers = outputs['danger_score'].cpu().numpy().flatten()
                
                # í˜„ì¬ ë°°ì¹˜ì—ì„œ ìµœëŒ€ ìœ„í—˜ë„ ì°¾ê¸°
                current_max = np.max(dangers)
                if current_max > max_danger:
                    max_danger = current_max
                    local_idx = np.argmax(dangers)
                    global_idx = (batch_idx * 64) + local_idx
                    if global_idx < len(pairs):
                        worst_pair = list(pairs[global_idx])
                batch_idx += 1
        
        print(f"ğŸ” [ê²°ê³¼] ìµœëŒ€ ìœ„í—˜ë„: {max_danger:.4f}, ì›ì¸: {worst_pair}")
        
        # 4. ì ìˆ˜ ê³„ì‚° (100 - ìœ„í—˜ë„*100)
        final_score = int(100 - (max_danger * 100))
        final_score = max(0, min(100, final_score))
        
        # 5. ìƒíƒœ ê²°ì • (60ì  ë¯¸ë§Œ ì£¼ì˜)
        status = "CAUTION" if final_score < 60 else "SAFE"
        
        # 6. ë²”ì¸ ëª©ë¡
        problematic_ingredients = []
        if status == "CAUTION":
            if worst_pair: problematic_ingredients = worst_pair
            else: problematic_ingredients = list(pairs[0]) # ì•ˆì „ì¥ì¹˜
        
        msg = []
        if status == "CAUTION":
            msg.append(f"ì£¼ì˜ê°€ í•„ìš”í•œ ì¡°í•©ì…ë‹ˆë‹¤. (ì•ˆì „ ì ìˆ˜: {final_score}ì )")
        else:
            msg.append(f"ì•ˆì „í•œ ì¡°í•©ì…ë‹ˆë‹¤. (ì•ˆì „ ì ìˆ˜: {final_score}ì )")
            
        return {
            "score": final_score,
            "status": status,
            "message": " ".join(msg),
            "problematic_ingredients": problematic_ingredients
        }
                
    except Exception as e:
        print(f"ì¶”ë¡  ì—ëŸ¬: {e}")
        return {"score": 0, "status": "UNKNOWN", "message": str(e)}

# â˜…â˜…â˜… [í•µì‹¬] OCR íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë° ê²°ê³¼ ì½ê¸° â˜…â˜…â˜…
def run_ocr_pipeline():
    if not os.path.exists(os.path.join(OCR_ROOT, "make_Polygon.py")):
        print("âš ï¸ OCR ìŠ¤í¬ë¦½íŠ¸ ì—†ìŒ. ê°€ì§œ ê²°ê³¼ ë°˜í™˜.")
        return ["ì •ì œìˆ˜", "ê¸€ë¦¬ì„¸ë¦°", "ë ˆí‹°ë†€", "ì—íƒ„ì˜¬"]

    try:
        python_exe = sys.executable
        
        print("ğŸ“¸ [1/3] Polygon ìƒì„±...")
        subprocess.run([python_exe, "make_Polygon.py"], cwd=OCR_ROOT, check=True)
        
        print("ğŸ“¸ [2/3] ì´ë¯¸ì§€ ìë¥´ê¸°...")
        subprocess.run([python_exe, "Crop_Polygons.py"], cwd=OCR_ROOT, check=True)
        
        print("ğŸ“¸ [3/3] í…ìŠ¤íŠ¸ ì¸ì‹...")
        subprocess.run([python_exe, "Images_to_LMDB_and_Inference.py"], cwd=OCR_ROOT, check=True)
        
        # ê²°ê³¼ ì½ê¸° (inference_results.txt ë˜ëŠ” ê°œë³„ txt íŒŒì¼)
        detected_ingredients = []
        if os.path.exists(OCR_RESULT_DIR):
            files = os.listdir(OCR_RESULT_DIR)
            print(f"ğŸ“‚ ê²°ê³¼ í´ë” íŒŒì¼ ëª©ë¡: {files}")
            
            for filename in files:
                if filename.endswith(".txt"):
                    try:
                        filepath = os.path.join(OCR_RESULT_DIR, filename)
                        with open(filepath, 'r', encoding='utf-8') as f:
                            lines = f.readlines()
                            for line in lines:
                                # íŒŒì¼ í˜•ì‹ì´ "ì´ë¯¸ì§€ê²½ë¡œ \t í…ìŠ¤íŠ¸ \t ì ìˆ˜" ì¸ ê²½ìš°ë¥¼ ëŒ€ë¹„
                                parts = line.strip().split('\t')
                                if len(parts) < 2: 
                                    parts = line.strip().split(',') # ì½¤ë§ˆ êµ¬ë¶„ ì‹œë„
                                
                                # í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë³´í†µ 2ë²ˆì§¸ë‚˜ ë§ˆì§€ë§‰ ì»¬ëŸ¼)
                                for part in parts:
                                    # í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆê±°ë‚˜ ê¸¸ì´ê°€ 2 ì´ìƒì¸ ê²ƒë§Œ ì¶”ì¶œ
                                    clean_text = part.strip()
                                    if len(clean_text) >= 2:
                                        detected_ingredients.append(clean_text)
                    except: pass
        
        unique_ingredients = list(set(detected_ingredients))
        print(f"ğŸ“¸ [OCR ì™„ë£Œ] ì¶”ì¶œëœ ë‹¨ì–´ {len(unique_ingredients)}ê°œ: {unique_ingredients[:10]}...")
        return unique_ingredients

    except Exception as e:
        print(f"âŒ OCR ì‹¤í–‰ ì—ëŸ¬: {e}")
        import traceback
        traceback.print_exc()
        return []

# ------------------------------------------------------
# [7] API ì—”ë“œí¬ì¸íŠ¸
# ------------------------------------------------------
def get_db_conn():
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    return conn

@app.route('/api/analyze/text', methods=['POST'])
def analyze_by_name():
    try:
        data = request.json
        p1, p2 = data.get('product1_name'), data.get('product2_name')
        ing1, ing2 = PRODUCT_DICT.get(p1), PRODUCT_DICT.get(p2)
        if not ing1 or not ing2: return jsonify({'error': 'ì œí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}), 404

        combined = list(set(ing1 + ing2))
        result = run_gnn_inference(combined)

        conn = get_db_conn()
        pid1, pid2, hid = str(uuid.uuid4()), str(uuid.uuid4()), str(uuid.uuid4())
        now = datetime.now().strftime("%Y-%m-%d")
        conn.execute("INSERT INTO user_products VALUES (?, ?, ?, ?)", (pid1, p1, json.dumps(ing1), now))
        conn.execute("INSERT INTO user_products VALUES (?, ?, ?, ?)", (pid2, p2, json.dumps(ing2), now))
        conn.execute("INSERT INTO analysis_history VALUES (?, ?, ?, ?)", (hid, f"{now} {datetime.now().strftime('%H:%M')}", json.dumps([p1, p2]), json.dumps(result)))
        conn.commit()
        conn.close()

        return jsonify({"products": [{"id": pid1, "name": p1, "ingredients": ing1}, {"id": pid2, "name": p2, "ingredients": ing2}], "analysis": result})
    except Exception as e: return jsonify({'error': str(e)}), 500

@app.route('/api/analyze/image', methods=['POST'])
def analyze_by_image():
    try:
        if 'image1' not in request.files or 'image2' not in request.files:
            return jsonify({'error': 'ì´ë¯¸ì§€ 2ê°œê°€ í•„ìš”í•©ë‹ˆë‹¤.'}), 400
            
        img1 = request.files['image1']
        img2 = request.files['image2']
        
        if os.path.exists(OCR_INPUT_DIR): shutil.rmtree(OCR_INPUT_DIR)
        os.makedirs(OCR_INPUT_DIR, exist_ok=True)
        
        img1.save(os.path.join(OCR_INPUT_DIR, "input_01.jpg"))
        img2.save(os.path.join(OCR_INPUT_DIR, "input_02.jpg"))
        
        # OCR ì‹¤í–‰
        ingredients = run_ocr_pipeline()
        
        # GNN ë¶„ì„ ì‹¤í–‰
        result = run_gnn_inference(ingredients)
        
        return jsonify({
            "products": [
                {"id": "img1", "name": "ì‚¬ì§„ ì œí’ˆ 1", "ingredients": []},
                {"id": "img2", "name": "ì‚¬ì§„ ì œí’ˆ 2", "ingredients": []}
            ],
            "analysis": result
        })
    except Exception as e:
        print(f"ì´ë¯¸ì§€ í•¸ë“¤ëŸ¬ ì—ëŸ¬: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/my-vanity', methods=['GET'])
def get_my_vanity():
    try:
        conn = get_db_conn()
        rows = conn.execute("SELECT * FROM user_products ORDER BY date DESC").fetchall()
        return jsonify([dict(r, ingredients=json.loads(r['ingredients'])) for r in rows])
    except: return jsonify([])

@app.route('/api/management', methods=['GET'])
def get_management():
    try:
        conn = get_db_conn()
        rows = conn.execute("SELECT * FROM analysis_history ORDER BY date DESC").fetchall()
        return jsonify([dict(r, items=json.loads(r['items']), result=json.loads(r['result'])) for r in rows])
    except: return jsonify([])

@app.route('/', defaults={'path': ''})
@app.route('/<path:path>')
def serve_react(path):
    if path.startswith("api/"): return jsonify({"error": "Not Found"}), 404
    if path != "" and os.path.exists(app.static_folder + '/' + path):
        return send_from_directory(app.static_folder, path)
    else:
        return send_from_directory(app.template_folder, 'index.html')

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8000, debug=False)
