"""
GNN (Graph Neural Network) ê¸°ë°˜ ì„±ë¶„ ì¡°í•© ë¶„ì„ ëª¨ë¸ - ë°œí‘œìš© ìµœì¢… ë²„ì „
ë‚ ì§œ: 2025-11-11
ê¸°ëŠ¥:
- Early Stopping
- Dropout
- ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ë¡œë“œ (ì¤‘ê°„ ì €ì¥ ë° ì¬ì‹œì‘ ì§€ì›)
- í•™ìŠµ ê³¡ì„  PNG ì €ì¥ (loss/accuracy)
- ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ PNG ì €ì¥
- ìƒì„¸í•œ ì§„í–‰ ìƒí™© í‘œì‹œ
- ì¤€ì§€ë„í•™ìŠµ ì„±ëŠ¥ í‰ê°€ ì§€í‘œ
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pickle
from typing import List, Dict, Tuple, Optional
from torch.utils.data import Dataset, DataLoader
from datetime import datetime
import os
import json
import warnings
import time
import matplotlib.pyplot as plt
from tqdm import tqdm
import sys
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'AppleGothic'  # macOS
# plt.rcParams['font.family'] = 'Malgun Gothic'  # Windows
# plt.rcParams['font.family'] = 'NanumGothic'  # Linux
plt.rcParams['axes.unicode_minus'] = False

# PyTorch Geometricì´ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ëŒ€ì²´ êµ¬í˜„
try:
    from torch_geometric.nn import GCNConv, GATConv, global_mean_pool
    from torch_geometric.data import Data, Batch
    PYG_AVAILABLE = True
    GAT_AVAILABLE = True
except ImportError:
    PYG_AVAILABLE = False
    GAT_AVAILABLE = False
    print("âš ï¸ PyTorch Geometricì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëŒ€ì²´ êµ¬í˜„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MAX_SEQ_LEN = 50  # ìµœëŒ€ ì„±ë¶„ ìˆ˜

if not PYG_AVAILABLE:
    # ëŒ€ì²´ êµ¬í˜„
    class GCNConv(nn.Module):
        def __init__(self, in_channels, out_channels):
            super().__init__()
            self.linear = nn.Linear(in_channels, out_channels)
        
        def forward(self, x, edge_index):
            return self.linear(x)
    
    class GATConv(nn.Module):
        def __init__(self, in_channels, out_channels, heads=4, dropout=0.0):
            super().__init__()
            self.linear = nn.Linear(in_channels, out_channels)
            self.heads = heads
        
        def forward(self, x, edge_index):
            return self.linear(x)
    
    def global_mean_pool(x, batch):
        return x.mean(dim=0, keepdim=True)
    
    class Data:
        def __init__(self, x=None, edge_index=None):
            self.x = x
            self.edge_index = edge_index
    
    class Batch:
        @staticmethod
        def from_data_list(data_list):
            return data_list


class IngredientFormulaDataset(Dataset):
    """ì„±ë¶„ í¬ë®¬ëŸ¬ ë°ì´í„°ì…‹ (GNNìš©) - ë¼ë²¨ ì—¬ë¶€ í¬í•¨"""
    
    def __init__(self, formulas: List[Tuple[List[str], float, float, bool]], vocab_to_idx: Dict[str, int]):
        self.formulas = formulas
        self.vocab_to_idx = vocab_to_idx
        
    def __len__(self):
        return len(self.formulas)
    
    def __getitem__(self, idx):
        ingredients, danger, synergy, has_label = self.formulas[idx]
        
        # ë…¸ë“œ íŠ¹ì§• (ì„±ë¶„ ID)
        node_ids = [self.vocab_to_idx.get(ing, 0) for ing in ingredients]
        
        # ì—£ì§€ ì¸ë±ìŠ¤ (ì™„ì „ ì—°ê²° ê·¸ë˜í”„)
        num_nodes = len(node_ids)
        edge_index = []
        for i in range(num_nodes):
            for j in range(num_nodes):
                edge_index.append([i, j])
        
        if len(edge_index) > 0:
            edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
        else:
            edge_index = torch.tensor([[], []], dtype=torch.long)
        
        # ë¼ë²¨ì´ ì—†ëŠ” ê²½ìš° (-1)ëŠ” 0ìœ¼ë¡œ ë³€í™˜í•˜ë˜, has_labelë¡œ êµ¬ë¶„
        danger_value = max(0.0, danger)  # -1 -> 0
        synergy_value = max(0.0, synergy)  # -1 -> 0
        
        return {
            'node_ids': torch.tensor(node_ids, dtype=torch.long),
            'edge_index': edge_index,
            'danger': torch.tensor(danger_value, dtype=torch.float),
            'synergy': torch.tensor(synergy_value, dtype=torch.float),
            'has_label': torch.tensor(1.0 if has_label else 0.0, dtype=torch.float),
            'num_nodes': num_nodes
        }


class GNNCollate:
    """GNN ë°°ì¹˜ ì²˜ë¦¬"""
    
    def __call__(self, batch):
        if PYG_AVAILABLE:
            data_list = []
            for item in batch:
                data = Data(
                    x=item['node_ids'].unsqueeze(1).float(),
                    edge_index=item['edge_index']
                )
                data.danger = item['danger']
                data.synergy = item['synergy']
                data.has_label = item['has_label']
                data_list.append(data)
            
            batch_data = Batch.from_data_list(data_list)
            return batch_data
        else:
            return batch


class GNNAnalyzerModel(nn.Module):
    """GNN ê¸°ë°˜ ì„±ë¶„ ì¡°í•© ë¶„ì„ ëª¨ë¸ (GAT ì‚¬ìš©, ì„ë² ë”© ì°¨ì› 512, Hidden 512)"""
    
    def __init__(self, vocab_size: int, embedding_dim: int = 512, hidden_dim: int = 512, 
                 num_layers: int = 3, dropout: float = 0.4, use_gat: bool = True, num_heads: int = 4):
        super().__init__()
        
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.dropout = dropout
        self.use_gat = use_gat and GAT_AVAILABLE
        
        # ì„ë² ë”© ë ˆì´ì–´
        self.ingredient_embedding = nn.Embedding(vocab_size, embedding_dim)
        
        # GNN ë ˆì´ì–´ (GAT ë˜ëŠ” GCN)
        self.gnn_layers = nn.ModuleList()
        
        if self.use_gat:
            # GAT ì²« ë ˆì´ì–´: embedding_dim -> hidden_dim (heads ê°œì˜ attention)
            self.gnn_layers.append(GATConv(embedding_dim, hidden_dim // num_heads, heads=num_heads, 
                                         dropout=dropout, concat=True))
            # GAT ì¤‘ê°„ ë ˆì´ì–´ë“¤
            for _ in range(num_layers - 2):
                self.gnn_layers.append(GATConv(hidden_dim, hidden_dim // num_heads, heads=num_heads, 
                                               dropout=dropout, concat=True))
            # GAT ë§ˆì§€ë§‰ ë ˆì´ì–´: concat=Falseë¡œ ì°¨ì› ìœ ì§€
            if num_layers > 1:
                self.gnn_layers.append(GATConv(hidden_dim, hidden_dim, heads=1, 
                                              dropout=dropout, concat=False))
        else:
            # GCN ì‚¬ìš©
            self.gnn_layers.append(GCNConv(embedding_dim, hidden_dim))
            for _ in range(num_layers - 1):
                self.gnn_layers.append(GCNConv(hidden_dim, hidden_dim))
        
        # ì¶œë ¥ í—¤ë“œ ê°œì„  (ë” ê¹Šì€ MLP)
        self.danger_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )
        
        self.synergy_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )
        
    def forward(self, batch_data):
        if PYG_AVAILABLE:
            x = self.ingredient_embedding(batch_data.x.squeeze().long())
            edge_index = batch_data.edge_index
            
            # GNN ë ˆì´ì–´ (Residual connection ê³ ë ¤)
            for i, gnn_layer in enumerate(self.gnn_layers):
                x_new = gnn_layer(x, edge_index)
                x_new = F.relu(x_new)
                x_new = F.dropout(x_new, p=self.dropout, training=self.training)
                
                # Residual connection (ì°¨ì›ì´ ê°™ì„ ë•Œë§Œ)
                if i > 0 and x.shape == x_new.shape:
                    x = x + x_new  # Residual connection
                else:
                    x = x_new
            
            # ê·¸ë˜í”„ í’€ë§
            batch = batch_data.batch if hasattr(batch_data, 'batch') else None
            if batch is not None:
                pooled = global_mean_pool(x, batch)
            else:
                pooled = x.mean(dim=0, keepdim=True)
        else:
            node_ids = batch_data[0]['node_ids']
            x = self.ingredient_embedding(node_ids)
            
            for i, gnn_layer in enumerate(self.gnn_layers):
                x_new = gnn_layer(x, batch_data[0]['edge_index'])
                x_new = F.relu(x_new)
                x_new = F.dropout(x_new, p=self.dropout, training=self.training)
                
                # Residual connection
                if i > 0 and x.shape == x_new.shape:
                    x = x + x_new
                else:
                    x = x_new
            
            pooled = x.mean(dim=0, keepdim=True)
        
        # ì¶œë ¥ í—¤ë“œ (BatchNormì€ ë°°ì¹˜ í¬ê¸°ê°€ 1ì¼ ë•Œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²˜ë¦¬)
        # BatchNorm ëŒ€ì‹  LayerNorm ì‚¬ìš©í•˜ê±°ë‚˜, ë°°ì¹˜ í¬ê¸°ê°€ 1ì¼ ë•ŒëŠ” BatchNormì„ ê±´ë„ˆë›°ê¸°
        if pooled.shape[0] == 1:
            # ë°°ì¹˜ í¬ê¸°ê°€ 1ì´ë©´ BatchNormì„ ê±´ë„ˆë›°ê³  ì§ì ‘ ê³„ì‚°
            # Sequential êµ¬ì¡°: Linear(0) -> BatchNorm(1) -> ReLU(2) -> Dropout(3) -> Linear(4) -> BatchNorm(5) -> ReLU(6) -> Dropout(7) -> Linear(8) -> Sigmoid(9)
            danger_score = self.danger_head[0](pooled)  # Linear
            danger_score = F.relu(danger_score)
            danger_score = F.dropout(danger_score, p=self.dropout, training=self.training)
            danger_score = self.danger_head[4](danger_score)  # Linear
            danger_score = F.relu(danger_score)
            danger_score = F.dropout(danger_score, p=self.dropout, training=self.training)
            danger_score = self.danger_head[8](danger_score)  # Linear
            danger_score = torch.sigmoid(danger_score)
            
            synergy_score = self.synergy_head[0](pooled)
            synergy_score = F.relu(synergy_score)
            synergy_score = F.dropout(synergy_score, p=self.dropout, training=self.training)
            synergy_score = self.synergy_head[4](synergy_score)
            synergy_score = F.relu(synergy_score)
            synergy_score = F.dropout(synergy_score, p=self.dropout, training=self.training)
            synergy_score = self.synergy_head[8](synergy_score)
            synergy_score = torch.sigmoid(synergy_score)
        else:
            danger_score = self.danger_head(pooled)
            synergy_score = self.synergy_head(pooled)
        
        return {
            'danger_score': danger_score,
            'synergy_score': synergy_score
        }


class GNNCosmeticAnalyzer:
    """GNN ê¸°ë°˜ í™”ì¥í’ˆ ì„±ë¶„ ë¶„ì„ê¸°"""
    
    def __init__(self, model_path: Optional[str] = None):
        self.model = None
        self.vocab = None
        self.vocab_to_idx = None
        self.idx_to_vocab = None
        self.model_path = model_path
        self.device = DEVICE

    def create_formulas_from_pairs(self, pairs: List[Tuple[str, str, float, float]]) -> List[Tuple[List[str], float, float, bool]]:
        """ì„±ë¶„ ìŒì„ í¬ë®¬ëŸ¬ë¡œ ë³€í™˜ (ë¼ë²¨ ì—¬ë¶€ í¬í•¨)"""
        formulas = []
        for ing1, ing2, danger, synergy in pairs:
            # ë¼ë²¨ì´ ìˆëŠ”ì§€ í™•ì¸ (danger ë˜ëŠ” synergyê°€ 0ë³´ë‹¤ í¬ë©´ ë¼ë²¨ ìˆìŒ)
            has_label = (danger > 0.0) or (synergy > 0.0)
            formulas.append(([ing1, ing2], danger, synergy, has_label))
        return formulas
    
    def _save_architecture_diagram(self, save_dir: str):
        """GNN ëª¨ë¸ì˜ ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ì„ PNGë¡œ ì €ì¥ (í•™ìŠµ ì „ì— í˜¸ì¶œ)"""
        os.makedirs(save_dir, exist_ok=True)
        fig, ax = plt.subplots(figsize=(12, 8))
        ax.axis('off')
        y = 0.95
        
        def box(text, y, color='#E3F2FD', height=0.08):
            ax.add_patch(plt.Rectangle((0.1, y-height), 0.8, height, color=color, ec='black', lw=2))
            ax.text(0.5, y-height/2, text, ha='center', va='center', fontsize=12, fontweight='bold')
            return y - height - 0.02
        
        def arrow(y):
            ax.annotate('', xy=(0.5, y-0.02), xytext=(0.5, y+0.02),
                       arrowprops=dict(arrowstyle='->', lw=2, color='black'))
            return y - 0.04
        
        y = box('ì…ë ¥: ì„±ë¶„ ëª©ë¡ (ë…¸ë“œ), ì™„ì „ ì—°ê²° ì—£ì§€', y, '#E3F2FD')
        y = arrow(y)
        y = box('ì„ë² ë”© ë ˆì´ì–´\n(Embedding, dim=512)', y, '#FFF3E0')
        y = arrow(y)
        y = box('GCN Layers\n(layers=2, dim=256, dropout=0.4)', y, '#E8F5E9')
        y = arrow(y)
        y = box('ê¸€ë¡œë²Œ í‰ê·  í’€ë§\n(Global Mean Pooling)', y, '#F3E5F5')
        y = arrow(y)
        y = box('Danger Head\nLinearâ†’ReLUâ†’Dropoutâ†’Sigmoid', y, '#FFEBEE')
        y = box('Synergy Head\nLinearâ†’ReLUâ†’Dropoutâ†’Sigmoid', y-0.1, '#FFEBEE')
        
        ax.text(0.5, 0.02, 'GNN ê¸°ë°˜ ì„±ë¶„ ì¡°í•© ë¶„ì„ ëª¨ë¸ ì•„í‚¤í…ì²˜', 
               ha='center', fontsize=14, fontweight='bold')
        
        out = os.path.join(save_dir, 'gnn_architecture.png')
        plt.savefig(out, dpi=200, bbox_inches='tight')
        plt.close()
        print(f"ğŸ§© ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ ì €ì¥: {out}")
        
    def save_checkpoint(self, save_dir: str, epoch: int, optimizer, scheduler, 
                       train_losses: List, val_losses: List, 
                       train_accuracies: List, val_accuracies: List,
                       best_val_loss: float, best_epoch: int, patience_counter: int,
                       use_weighted_loss: bool = False):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        os.makedirs(save_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        checkpoint_path = os.path.join(save_dir, f"checkpoint_epoch{epoch+1}_{timestamp}.pth")
        
        checkpoint = {
            'epoch': epoch + 1,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'vocab': self.vocab,
            'vocab_to_idx': self.vocab_to_idx,
            'idx_to_vocab': self.idx_to_vocab,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'train_accuracies': train_accuracies,
            'val_accuracies': val_accuracies,
            'best_val_loss': best_val_loss,
            'best_epoch': best_epoch,
            'patience_counter': patience_counter,
            'use_weighted_loss': use_weighted_loss,
            'timestamp': timestamp
        }
        
        torch.save(checkpoint, checkpoint_path)
        return checkpoint_path
    
    def load_checkpoint(self, checkpoint_path: str, optimizer=None, scheduler=None):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        self.vocab = checkpoint['vocab']
        self.vocab_to_idx = checkpoint['vocab_to_idx']
        self.idx_to_vocab = checkpoint['idx_to_vocab']
        
        vocab_size = len(self.vocab)
        self.model = GNNAnalyzerModel(
            vocab_size=vocab_size,
            embedding_dim=512,
            hidden_dim=512,  # 256 â†’ 512ë¡œ ì¦ê°€
            num_layers=3,  # 2 â†’ 3ìœ¼ë¡œ ì¦ê°€
            dropout=0.4,
            use_gat=True,
            num_heads=4
        ).to(self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        if optimizer is not None:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        if scheduler is not None:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: {checkpoint_path}")
        print(f"   - ì—í­: {checkpoint['epoch']}")
        print(f"   - Best Val Loss: {checkpoint['best_val_loss']:.4f}")
        print(f"   - Best Epoch: {checkpoint['best_epoch']}")
        
        return {
            'epoch': checkpoint['epoch'],
            'train_losses': checkpoint.get('train_losses', []),
            'val_losses': checkpoint.get('val_losses', []),
            'train_accuracies': checkpoint.get('train_accuracies', []),
            'val_accuracies': checkpoint.get('val_accuracies', []),
            'best_val_loss': checkpoint['best_val_loss'],
            'best_epoch': checkpoint['best_epoch'],
            'patience_counter': checkpoint.get('patience_counter', 0)
        }
    
    def train(self,
              train_data: List[Tuple[str, str, float, float]],
              val_data: List[Tuple[str, str, float, float]],
              vocab: List[str],
              vocab_to_idx: Dict[str, int],
              num_epochs: int = 30,
              batch_size: int = 64,
              learning_rate: float = 0.001,
              save_dir: str = "models/trained/gnn",
              save_plots: bool = True,
              early_stopping_patience: int = 10,
              early_stopping_min_delta: float = 0.001,
              checkpoint_interval: int = 5,
              resume_from_checkpoint: Optional[str] = None):
        """ëª¨ë¸ í•™ìŠµ"""
        print(f"ğŸš€ GNN ëª¨ë¸ í•™ìŠµ ì‹œì‘ (Device: {self.device})...")
        
        # ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ ì €ì¥ (í•™ìŠµ ì „)
        try:
            self._save_architecture_diagram(save_dir)
        except Exception as e:
            print(f"âš ï¸ ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        self.vocab = vocab
        self.vocab_to_idx = vocab_to_idx
        self.idx_to_vocab = {idx: ing for ing, idx in vocab_to_idx.items()}
        
        # í¬ë®¬ëŸ¬ë¡œ ë³€í™˜
        train_formulas = self.create_formulas_from_pairs(train_data)
        val_formulas = self.create_formulas_from_pairs(val_data)
        
        # ë°ì´í„°ì…‹ ìƒì„±
        train_dataset = IngredientFormulaDataset(train_formulas, vocab_to_idx)
        val_dataset = IngredientFormulaDataset(val_formulas, vocab_to_idx)
        
        collate_fn = GNNCollate()
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
        
        # ëª¨ë¸ ì´ˆê¸°í™” (GAT ì‚¬ìš©, ì„ë² ë”© 512, Hidden 512, ë ˆì´ì–´ 3)
        vocab_size = len(vocab)
        self.model = GNNAnalyzerModel(
            vocab_size=vocab_size,
            embedding_dim=512,
            hidden_dim=512,  # ì„ë² ë”©ê³¼ ë™ì¼í•˜ê²Œ ì¦ê°€
            num_layers=3,  # 2 â†’ 3ìœ¼ë¡œ ì¦ê°€
            dropout=0.4,
            use_gat=True,  # GAT ì‚¬ìš©
            num_heads=4  # Attention heads
        ).to(self.device)
        
        print(f"   âœ… GAT (Graph Attention Network) ì‚¬ìš©: {self.model.use_gat}")
        print(f"   âœ… ëª¨ë¸ êµ¬ì¡°: Embedding(512) â†’ GAT Layers(3) â†’ Hidden(512) â†’ Output")
        
        # ë°ì´í„° ë¶„í¬ í™•ì¸ (ë¼ë²¨ì´ ìˆëŠ” ë°ì´í„°ë§Œ ë¶„ì„)
        labeled_data = [(d, s) for _, _, d, s in train_data if d > 0.0 or s > 0.0]
        unlabeled_count = len(train_data) - len(labeled_data)
        
        danger_count = sum(1 for d, _ in labeled_data if d > 0.0)
        synergy_count = sum(1 for _, s in labeled_data if s > 0.0)
        both_count = sum(1 for d, s in labeled_data if d > 0.0 and s > 0.0)
        
        print(f"\nğŸ“Š ë°ì´í„° ë¶„í¬ í™•ì¸:")
        print(f"   - ë¼ë²¨ì´ ìˆëŠ” ì¡°í•©: {len(labeled_data)}ê°œ (ì •ë‹µ ë ˆì´ë¸”)")
        print(f"     * ìœ„í—˜í•œ ì¡°í•©: {danger_count}ê°œ")
        print(f"     * ì‹œë„ˆì§€ ì¡°í•©: {synergy_count}ê°œ")
        print(f"     * ìœ„í—˜+ì‹œë„ˆì§€ ë™ì‹œ: {both_count}ê°œ")
        print(f"   - ë¼ë²¨ì´ ì—†ëŠ” ì¡°í•©: {unlabeled_count}ê°œ (ë¯¸í™•ì¸ ìƒíƒœ)")
        print(f"   ğŸ’¡ ìš°ì„ ìˆœìœ„: ìœ„í—˜(1ìˆœìœ„) > ì‹œë„ˆì§€(2ìˆœìœ„) > ì•ˆì „(3ìˆœìœ„)")
        print(f"   ğŸ’¡ ë¼ë²¨ì´ ì—†ëŠ” ì¡°í•©ì€ ì†ì‹¤ ê³„ì‚°ì—ì„œ ì œì™¸ë©ë‹ˆë‹¤.")
        
        # ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤ í•¨ìˆ˜ (í•™ìŠµë¥  ì¡°ì • - nan ë°©ì§€í•˜ë©´ì„œë„ í•™ìŠµ ì†ë„ í™•ë³´)
        # Lossê°€ ì•ˆì •í™”ë˜ì—ˆìœ¼ë¯€ë¡œ í•™ìŠµë¥ ì„ ì†Œí­ ìƒí–¥ (0.0001 â†’ 0.0005)
        safe_learning_rate = min(learning_rate, 0.0005)  # ìµœëŒ€ 0.0005ë¡œ ì œí•œ (ì•ˆì •í™” í›„ ìƒí–¥)
        if learning_rate > 0.0005:
            print(f"   âš ï¸ í•™ìŠµë¥ ì´ ë„ˆë¬´ ë†’ìŠµë‹ˆë‹¤. {learning_rate} â†’ {safe_learning_rate}ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.")
        optimizer = torch.optim.Adam(self.model.parameters(), lr=safe_learning_rate, weight_decay=1e-5)
        
        # ìœ„í—˜í•œ ì¡°í•©ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬ (ìš°ì„ ìˆœìœ„ 1ìˆœìœ„) - ê·¹ë‹¨ì ìœ¼ë¡œ ë†’ê²Œ ì„¤ì •
        if len(labeled_data) > 0 and danger_count > 0:
            # ìœ„í—˜í•œ ì¡°í•©ì— ê·¹ë‹¨ì ìœ¼ë¡œ ë†’ì€ ê°€ì¤‘ì¹˜ (10-20 ì´ìƒ)
            danger_ratio = danger_count / len(labeled_data)
            base_pos_weight = (1.0 - danger_ratio) / danger_ratio
            # ìµœì†Œ 15.0 ì´ìƒìœ¼ë¡œ ì„¤ì • (ìœ„í—˜ í´ë˜ìŠ¤ë¥¼ ê°•ì œë¡œ í•™ìŠµ)
            pos_weight = torch.tensor([max(15.0, base_pos_weight * 2.0)]).to(self.device)
            bce_criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
            print(f"   âœ… ìœ„í—˜ë„ ë¶„ë¥˜ì— í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©: {pos_weight.item():.2f} (ê·¹ë‹¨ì ìœ¼ë¡œ ë†’ê²Œ ì„¤ì •)")
        else:
            bce_criterion = nn.BCELoss()
        
        mse_criterion = nn.MSELoss()
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, 
            mode='min', 
            factor=0.5, 
            patience=3, 
            min_lr=1e-6
        )
        
        # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ì‹œì‘
        start_epoch = 0
        train_losses = []
        val_losses = []
        train_accuracies = []
        val_accuracies = []
        best_val_loss = float('inf')
        best_epoch = 0
        patience_counter = 0
        
        if resume_from_checkpoint:
            print(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ì‹œì‘: {resume_from_checkpoint}")
            checkpoint_data = self.load_checkpoint(resume_from_checkpoint, optimizer, scheduler)
            start_epoch = checkpoint_data['epoch']
            train_losses = checkpoint_data['train_losses']
            val_losses = checkpoint_data['val_losses']
            train_accuracies = checkpoint_data['train_accuracies']
            val_accuracies = checkpoint_data['val_accuracies']
            best_val_loss = checkpoint_data['best_val_loss']
            best_epoch = checkpoint_data['best_epoch']
            patience_counter = checkpoint_data['patience_counter']
            print(f"   ì¬ì‹œì‘ ì—í­: {start_epoch}/{num_epochs}")
        
        print(f"   - ì´ ì—í­ ìˆ˜: {num_epochs}")
        print(f"   - ì‹œì‘ ì—í­: {start_epoch}")
        print(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"   - í•™ìŠµë¥ : {safe_learning_rate} (ì›ë˜: {learning_rate})")
        print(f"   - Early Stopping Patience: {early_stopping_patience}")
        print(f"   - Early Stopping Min Delta: {early_stopping_min_delta}")
        print(f"   - ì²´í¬í¬ì¸íŠ¸ ê°„ê²©: {checkpoint_interval} ì—í­")
        print(f"   - ìœ„í—˜ë„ ì†ì‹¤ ê°€ì¤‘ì¹˜: 100.0 (ê·¹ë‹¨ì ìœ¼ë¡œ ë†’ê²Œ ì„¤ì •)")
        print(f"   - Dropout: 0.4 (ê³¼ì í•© ë°©ì§€ ê°•í™”)")
        print(f"   - Gradient Clipping: 1.0 (nan ë°©ì§€)")
        
        total_start_time = time.time()
        
        # í•™ìŠµ ë£¨í”„
        for epoch in range(start_epoch, num_epochs):
            epoch_start_time = time.time()
            
            # í›ˆë ¨
            self.model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0
            
            train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Train]", 
                            leave=False, ncols=100)
            for batch_idx, batch in enumerate(train_pbar):
                if PYG_AVAILABLE:
                    danger_target = batch.danger.to(self.device)
                    synergy_target = batch.synergy.to(self.device)
                    has_label = batch.has_label.to(self.device)
                else:
                    danger_target = torch.stack([item['danger'] for item in batch]).to(self.device)
                    synergy_target = torch.stack([item['synergy'] for item in batch]).to(self.device)
                    has_label = torch.stack([item['has_label'] for item in batch]).to(self.device)
                
                # ìˆœì „íŒŒ
                outputs = self.model(batch)
                
                # ë¼ë²¨ì´ ìˆëŠ” ë°ì´í„°ë§Œ ì†ì‹¤ ê³„ì‚° (ìš°ì„ ìˆœìœ„: ìœ„í—˜ > ì‹œë„ˆì§€)
                label_mask = has_label > 0.5
                
                if label_mask.sum() > 0:
                    # ìœ„í—˜ë„ ì†ì‹¤ (1ìˆœìœ„) - ë¼ë²¨ì´ ìˆëŠ” ë°ì´í„°ë§Œ
                    danger_pred = outputs['danger_score'].squeeze()[label_mask]
                    danger_tgt = danger_target[label_mask]
                    
                    if isinstance(bce_criterion, nn.BCEWithLogitsLoss):
                        # Sigmoid ì¶œë ¥ì„ logitìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜ (nan ë°©ì§€)
                        # í´ë¦¬í•‘í•˜ì—¬ 0ê³¼ 1ì— ë„ˆë¬´ ê°€ê¹Œìš´ ê°’ ë°©ì§€
                        danger_pred_clipped = torch.clamp(danger_pred, min=1e-7, max=1-1e-7)
                        danger_logits = torch.log(danger_pred_clipped / (1 - danger_pred_clipped))
                        danger_loss = bce_criterion(danger_logits, danger_tgt)
                    else:
                        danger_loss = bce_criterion(danger_pred, danger_tgt)
                    
                    # Lossê°€ nanì¸ì§€ í™•ì¸
                    if torch.isnan(danger_loss) or torch.isinf(danger_loss):
                        print(f"  âš ï¸ ê²½ê³ : Danger Lossê°€ nan/infì…ë‹ˆë‹¤. ì´ ë°°ì¹˜ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue
                    
                    # ì‹œë„ˆì§€ ì†ì‹¤ (2ìˆœìœ„) - ìœ„í—˜ì´ ì•„ë‹Œ ê²½ìš°ë§Œ, ë¼ë²¨ì´ ìˆëŠ” ë°ì´í„°ë§Œ
                    synergy_mask = (danger_tgt == 0.0)  # ìœ„í—˜ì´ ì•„ë‹Œ ê²½ìš°
                    if synergy_mask.sum() > 0:
                        synergy_pred = outputs['synergy_score'].squeeze()[label_mask][synergy_mask]
                        synergy_tgt = synergy_target[label_mask][synergy_mask]
                        synergy_loss = mse_criterion(synergy_pred, synergy_tgt)
                        
                        # ì‹œë„ˆì§€ Lossê°€ nanì¸ì§€ í™•ì¸
                        if torch.isnan(synergy_loss) or torch.isinf(synergy_loss):
                            synergy_loss = torch.tensor(0.0, device=self.device)
                    else:
                        synergy_loss = torch.tensor(0.0, device=self.device)
                    
                    # ìš°ì„ ìˆœìœ„ ë°˜ì˜: ìœ„í—˜(1ìˆœìœ„) > ì‹œë„ˆì§€(2ìˆœìœ„) - ê°€ì¤‘ì¹˜ ê·¹ë‹¨ì ìœ¼ë¡œ ì¦ê°€
                    loss = 100.0 * danger_loss + 1.0 * synergy_loss
                    
                    # Lossê°€ nanì¸ì§€ í™•ì¸
                    if torch.isnan(loss) or torch.isinf(loss):
                        print(f"  âš ï¸ ê²½ê³ : Lossê°€ nan/infì…ë‹ˆë‹¤. ì´ ë°°ì¹˜ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue
                    
                    # ì—­ì „íŒŒ
                    optimizer.zero_grad()
                    loss.backward()
                    
                    # Gradient Clipping ì ìš© (nan ë°©ì§€)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    
                    optimizer.step()
                else:
                    # ë¼ë²¨ì´ ì—†ëŠ” ë°°ì¹˜ì˜ ê²½ìš° í•™ìŠµí•˜ì§€ ì•ŠìŒ (ì—­ì „íŒŒ ê±´ë„ˆë›°ê¸°)
                    loss = torch.tensor(0.0, device=self.device)
                    danger_loss = torch.tensor(0.0, device=self.device)
                    synergy_loss = torch.tensor(0.0, device=self.device)
                
                train_loss += loss.item()
                
                # ì •í™•ë„ ê³„ì‚° (ë¼ë²¨ì´ ìˆëŠ” ë°ì´í„°ë§Œ, ë°°ì¹˜ë³„ í‘œì‹œ ì œê±°)
                if label_mask.sum() > 0:
                    # ì„ê³„ê°’ 0.5 ì‚¬ìš© (í•™ìŠµ ì¤‘ì—ëŠ” ê³ ì •)
                    danger_pred = (outputs['danger_score'].squeeze()[label_mask] > 0.5).float()
                    train_correct += (danger_pred == danger_target[label_mask]).sum().item()
                    train_total += label_mask.sum().item()
                
                # ì§„í–‰ ìƒí™© í‘œì‹œ (ì •í™•ë„ ì œê±° - ë¼ë²¨ì´ í¬ì†Œí•˜ì—¬ ë°°ì¹˜ë³„ ì •í™•ë„ëŠ” ì˜ë¯¸ ì—†ìŒ)
                progress = (batch_idx + 1) / len(train_loader) * 100
                elapsed = time.time() - epoch_start_time
                avg_time_per_batch = elapsed / (batch_idx + 1)
                remaining_batches = len(train_loader) - (batch_idx + 1)
                remaining_time = avg_time_per_batch * remaining_batches
                
                train_pbar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'progress': f'{progress:.1f}%',
                    'remaining': f'{remaining_time:.0f}s'
                })
            
            # ê²€ì¦
            self.model.eval()
            val_loss = 0.0
            val_correct = 0
            val_total = 0
            
            val_pbar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Val]", 
                          leave=False, ncols=100)
            with torch.no_grad():
                for batch_idx, batch in enumerate(val_pbar):
                    if PYG_AVAILABLE:
                        danger_target = batch.danger.to(self.device)
                        synergy_target = batch.synergy.to(self.device)
                        has_label = batch.has_label.to(self.device)
                    else:
                        danger_target = torch.stack([item['danger'] for item in batch]).to(self.device)
                        synergy_target = torch.stack([item['synergy'] for item in batch]).to(self.device)
                        has_label = torch.stack([item['has_label'] for item in batch]).to(self.device)
                    
                    outputs = self.model(batch)
                    
                    # ë¼ë²¨ì´ ìˆëŠ” ë°ì´í„°ë§Œ ì†ì‹¤ ê³„ì‚°
                    label_mask = has_label > 0.5
                    
                    if label_mask.sum() > 0:
                        # ìœ„í—˜ë„ ì†ì‹¤
                        danger_pred = outputs['danger_score'].squeeze()[label_mask]
                        danger_tgt = danger_target[label_mask]
                        
                        if isinstance(bce_criterion, nn.BCEWithLogitsLoss):
                            # Sigmoid ì¶œë ¥ì„ logitìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜ (nan ë°©ì§€)
                            danger_pred_clipped = torch.clamp(danger_pred, min=1e-7, max=1-1e-7)
                            danger_logits = torch.log(danger_pred_clipped / (1 - danger_pred_clipped))
                            danger_loss = bce_criterion(danger_logits, danger_tgt)
                        else:
                            danger_loss = bce_criterion(danger_pred, danger_tgt)
                        
                        # Lossê°€ nanì¸ì§€ í™•ì¸
                        if torch.isnan(danger_loss) or torch.isinf(danger_loss):
                            danger_loss = torch.tensor(0.0, device=self.device)
                        
                        # ì‹œë„ˆì§€ ì†ì‹¤ (ìœ„í—˜ì´ ì•„ë‹Œ ê²½ìš°ë§Œ)
                        synergy_mask = (danger_tgt == 0.0)  # ìœ„í—˜ì´ ì•„ë‹Œ ê²½ìš°
                        if synergy_mask.sum() > 0:
                            synergy_pred = outputs['synergy_score'].squeeze()[label_mask][synergy_mask]
                            synergy_tgt = synergy_target[label_mask][synergy_mask]
                            synergy_loss = mse_criterion(synergy_pred, synergy_tgt)
                            
                            # ì‹œë„ˆì§€ Lossê°€ nanì¸ì§€ í™•ì¸
                            if torch.isnan(synergy_loss) or torch.isinf(synergy_loss):
                                synergy_loss = torch.tensor(0.0, device=self.device)
                        else:
                            synergy_loss = torch.tensor(0.0, device=self.device)
                        
                        loss = 100.0 * danger_loss + 1.0 * synergy_loss
                        
                        # Lossê°€ nanì¸ì§€ í™•ì¸
                        if torch.isnan(loss) or torch.isinf(loss):
                            loss = torch.tensor(0.0, device=self.device)
                    else:
                        loss = torch.tensor(0.0, device=self.device)
                        danger_loss = torch.tensor(0.0, device=self.device)
                        synergy_loss = torch.tensor(0.0, device=self.device)
                    
                    val_loss += loss.item()
                    
                    # ì •í™•ë„ ê³„ì‚° (ë¼ë²¨ì´ ìˆëŠ” ë°ì´í„°ë§Œ)
                    if label_mask.sum() > 0:
                        danger_pred_binary = (outputs['danger_score'].squeeze()[label_mask] > 0.5).float()
                        val_correct += (danger_pred_binary == danger_target[label_mask]).sum().item()
                        val_total += label_mask.sum().item()
                    
                    progress = (batch_idx + 1) / len(val_loader) * 100
                    # ê²€ì¦ ì •í™•ë„ëŠ” ì—í­ ì¢…ë£Œ ì‹œì—ë§Œ í‘œì‹œ (ë°°ì¹˜ë³„ í‘œì‹œ ì œê±°)
                    val_pbar.set_postfix({
                        'loss': f'{loss.item():.4f}',
                        'progress': f'{progress:.1f}%'
                    })
            
            train_loss /= len(train_loader)
            val_loss /= len(val_loader)
            # ì •í™•ë„ ê³„ì‚° (ZeroDivisionError ë°©ì§€)
            train_acc = train_correct / train_total if train_total > 0 else 0.0
            val_acc = val_correct / val_total if val_total > 0 else 0.0
            
            # ë¼ë²¨ì´ ìˆëŠ” ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ê²½ê³ 
            if train_total == 0:
                print(f"  âš ï¸ ê²½ê³ : ì´ë²ˆ ì—í­ì—ì„œ ë¼ë²¨ì´ ìˆëŠ” í›ˆë ¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            if val_total == 0:
                print(f"  âš ï¸ ê²½ê³ : ì´ë²ˆ ì—í­ì—ì„œ ë¼ë²¨ì´ ìˆëŠ” ê²€ì¦ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            train_losses.append(train_loss)
            val_losses.append(val_loss)
            train_accuracies.append(train_acc)
            val_accuracies.append(val_acc)
            
            epoch_time = time.time() - epoch_start_time
            elapsed_time = time.time() - total_start_time
            avg_time_per_epoch = elapsed_time / (epoch + 1 - start_epoch)
            remaining_epochs = num_epochs - (epoch + 1)
            estimated_remaining = avg_time_per_epoch * remaining_epochs
            
            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸ (Lossê°€ nanì´ ì•„ë‹Œ ê²½ìš°ë§Œ)
            if not (torch.isnan(torch.tensor(val_loss)) or torch.isinf(torch.tensor(val_loss))):
                scheduler.step(val_loss)
            
            # Early Stopping ì²´í¬ (Lossê°€ nanì´ ì•„ë‹Œ ê²½ìš°ë§Œ)
            improved = False
            if not (torch.isnan(torch.tensor(val_loss)) or torch.isinf(torch.tensor(val_loss))):
                if val_loss < best_val_loss - early_stopping_min_delta:
                    best_val_loss = val_loss
                    best_epoch = epoch + 1
                    patience_counter = 0
                    improved = True
                else:
                    patience_counter += 1
            else:
                # Lossê°€ nanì´ë©´ ê°œì„  ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬
                patience_counter += 1
                print(f"  âš ï¸ ê²½ê³ : Val Lossê°€ nan/infì…ë‹ˆë‹¤. ê°œì„  ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            
            # ì—í­ ì¢…ë£Œ ì‹œ ê°„ë‹¨í•œ ìš”ì•½ë§Œ ì¶œë ¥ (val_accë§Œ í‘œì‹œ - ë¼ë²¨ì´ í¬ì†Œí•˜ì—¬ train_accëŠ” ì˜ë¯¸ ì—†ìŒ)
            status = "âœ… ê°œì„ " if improved else f"â³ ëŒ€ê¸° ({patience_counter}/{early_stopping_patience})"
            if val_total > 0:
                print(f"\nEpoch {epoch+1}/{num_epochs} | Loss: {train_loss:.4f}/{val_loss:.4f} | "
                      f"Val Acc: {val_acc*100:.1f}% | {status} | "
                      f"ë‚¨ì€ ì‹œê°„: {estimated_remaining/60:.1f}ë¶„")
            else:
                print(f"\nEpoch {epoch+1}/{num_epochs} | Loss: {train_loss:.4f}/{val_loss:.4f} | "
                      f"Val Acc: N/A (ë¼ë²¨ ì—†ìŒ) | {status} | "
                      f"ë‚¨ì€ ì‹œê°„: {estimated_remaining/60:.1f}ë¶„")
            
            # ì¤‘ê°„ ê²°ê³¼ ì‹œê°í™” (ë§¤ 5 ì—í­ë§ˆë‹¤)
            if save_plots and (epoch + 1) % 5 == 0:
                self._plot_training_progress(train_losses, val_losses, train_accuracies, val_accuracies, 
                                            epoch + 1, save_dir)
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì§€ì •ëœ ê°„ê²©ë§ˆë‹¤)
            if (epoch + 1) % checkpoint_interval == 0:
                checkpoint_path = self.save_checkpoint(
                    save_dir, epoch, optimizer, scheduler,
                    train_losses, val_losses, train_accuracies, val_accuracies,
                    best_val_loss, best_epoch, patience_counter,
                    use_weighted_loss=isinstance(bce_criterion, nn.BCEWithLogitsLoss)
                )
                print(f"  ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
            
            # Early Stopping ì²´í¬
            if patience_counter >= early_stopping_patience:
                print(f"\nâ¹ï¸  Early Stopping! {early_stopping_patience} ì—í­ ë™ì•ˆ ê°œì„ ì´ ì—†ì–´ í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                print(f"   ìµœê³  ì„±ëŠ¥: Val Loss {best_val_loss:.4f} (Epoch {best_epoch})")
                break
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        total_time = time.time() - total_start_time
        print(f"\nâœ… GNN ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! (ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„)")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        final_model_path = os.path.join(save_dir, f"gnn_model_final_{timestamp}.pth")
        
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'vocab': self.vocab,
            'vocab_to_idx': self.vocab_to_idx,
            'idx_to_vocab': self.idx_to_vocab,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'train_accuracies': train_accuracies,
            'val_accuracies': val_accuracies,
            'best_val_loss': best_val_loss,
            'best_epoch': best_epoch,
            'timestamp': timestamp
        }, final_model_path)
        
        print(f"âœ… ìµœì¢… ëª¨ë¸ ì €ì¥: {final_model_path}")
        
        # ìµœì¢… í•™ìŠµ ê³¡ì„  ì €ì¥
        if save_plots:
            self._plot_training_progress(train_losses, val_losses, train_accuracies, val_accuracies, 
                                        len(train_losses), save_dir, final=True)
        
        return train_losses, val_losses, train_accuracies, val_accuracies
    
    def _plot_training_progress(self, train_losses, val_losses, train_accs, val_accs, 
                               current_epoch, save_dir, final=False):
        """í•™ìŠµ ì§„í–‰ ìƒí™© ì‹œê°í™”"""
        os.makedirs(save_dir, exist_ok=True)
        
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        epochs = range(1, len(train_losses) + 1)
        
        # ì†ì‹¤ ê·¸ë˜í”„
        axes[0].plot(epochs, train_losses, 'b-', label='Train Loss', linewidth=2, marker='o', markersize=4)
        axes[0].plot(epochs, val_losses, 'r-', label='Val Loss', linewidth=2, marker='s', markersize=4)
        axes[0].set_xlabel('Epoch', fontsize=12)
        axes[0].set_ylabel('Loss', fontsize=12)
        axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # ì •í™•ë„ ê·¸ë˜í”„
        axes[1].plot(epochs, [acc*100 for acc in train_accs], 'b-', label='Train Accuracy', 
                    linewidth=2, marker='o', markersize=4)
        axes[1].plot(epochs, [acc*100 for acc in val_accs], 'r-', label='Val Accuracy', 
                    linewidth=2, marker='s', markersize=4)
        axes[1].set_xlabel('Epoch', fontsize=12)
        axes[1].set_ylabel('Accuracy (%)', fontsize=12)
        axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        axes[1].set_ylim([0, 100])
        
        plt.tight_layout()
        
        if final:
            filename = os.path.join(save_dir, "gnn_training_final.png")
        else:
            filename = os.path.join(save_dir, f"gnn_training_epoch{current_epoch}.png")
        
        plt.savefig(filename, dpi=150, bbox_inches='tight')
        plt.close()
        
        if final:
            print(f"ğŸ“Š ìµœì¢… í•™ìŠµ ê³¡ì„  ì €ì¥: {filename}")
    
    def load_model(self, model_path: str):
        """ëª¨ë¸ ë¡œë“œ"""
        checkpoint = torch.load(model_path, map_location=self.device)
        
        self.vocab = checkpoint['vocab']
        self.vocab_to_idx = checkpoint['vocab_to_idx']
        self.idx_to_vocab = checkpoint['idx_to_vocab']
        
        vocab_size = len(self.vocab)
        self.model = GNNAnalyzerModel(
            vocab_size=vocab_size,
            embedding_dim=512,
            hidden_dim=512,  # 256 â†’ 512ë¡œ ì¦ê°€
            num_layers=3,  # 2 â†’ 3ìœ¼ë¡œ ì¦ê°€
            dropout=0.4,
            use_gat=True,
            num_heads=4
        ).to(self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
    
    def evaluate(self, test_data: List[Tuple[str, str, float, float]], batch_size: int = 32, 
                 danger_threshold: float = 0.5):
        """ëª¨ë¸ í‰ê°€ - ì¤€ì§€ë„í•™ìŠµì— ë§ëŠ” ì„±ëŠ¥ ì§€í‘œ (ë¼ë²¨ì´ ìˆëŠ” ë°ì´í„°ë§Œ í‰ê°€)
        
        Args:
            test_data: í…ŒìŠ¤íŠ¸ ë°ì´í„°
            batch_size: ë°°ì¹˜ í¬ê¸°
            danger_threshold: ìœ„í—˜ë„ ë¶„ë¥˜ ì„ê³„ê°’ (ê¸°ë³¸ 0.5, ë‚®ì¶”ë©´ Recall ì¦ê°€, Precision ê°ì†Œ)
        """
        if self.model is None:
            raise ValueError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
        print("   âš ï¸  ë¼ë²¨ì´ ìˆëŠ” ë°ì´í„°ë§Œ í‰ê°€í•©ë‹ˆë‹¤.")
        
        test_formulas = self.create_formulas_from_pairs(test_data)
        test_dataset = IngredientFormulaDataset(test_formulas, self.vocab_to_idx)
        collate_fn = GNNCollate()
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
        
        self.model.eval()
        all_danger_preds = []
        all_synergy_preds = []
        all_danger_targets = []
        all_synergy_targets = []
        
        with torch.no_grad():
            for batch in tqdm(test_loader, desc="í‰ê°€ ì¤‘", leave=False):
                if PYG_AVAILABLE:
                    danger_target = batch.danger.to(self.device)
                    synergy_target = batch.synergy.to(self.device)
                    has_label = batch.has_label.to(self.device)
                else:
                    danger_target = torch.stack([item['danger'] for item in batch]).to(self.device)
                    synergy_target = torch.stack([item['synergy'] for item in batch]).to(self.device)
                    has_label = torch.stack([item['has_label'] for item in batch]).to(self.device)
                
                outputs = self.model(batch)
                
                # ë¼ë²¨ì´ ìˆëŠ” ë°ì´í„°ë§Œ í‰ê°€
                label_mask = has_label > 0.5
                if label_mask.sum() > 0:
                    all_danger_preds.extend(outputs['danger_score'].cpu().numpy().flatten()[label_mask.cpu().numpy()])
                    all_synergy_preds.extend(outputs['synergy_score'].cpu().numpy().flatten()[label_mask.cpu().numpy()])
                    all_danger_targets.extend(danger_target[label_mask].cpu().numpy())
                    all_synergy_targets.extend(synergy_target[label_mask].cpu().numpy())
        
        all_danger_preds = np.array(all_danger_preds)
        all_synergy_preds = np.array(all_synergy_preds)
        all_danger_targets = np.array(all_danger_targets)
        all_synergy_targets = np.array(all_synergy_targets)
        
        # ì¤€ì§€ë„í•™ìŠµ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        # 1. ìœ„í—˜ë„ ë¶„ë¥˜ ì •í™•ë„ (0-100%) - ì„ê³„ê°’ ì¡°ì • ê°€ëŠ¥
        print(f"   ğŸ“Œ ìœ„í—˜ë„ ë¶„ë¥˜ ì„ê³„ê°’: {danger_threshold} (ê¸°ë³¸ 0.5, ë‚®ì¶”ë©´ Recall ì¦ê°€)")
        danger_preds_binary = (all_danger_preds > danger_threshold).astype(int)
        danger_targets_binary = (all_danger_targets > 0.5).astype(int)
        danger_accuracy = (danger_preds_binary == danger_targets_binary).mean() * 100
        
        # 2. ìœ„í—˜ë„ Precision, Recall, F1
        true_positives = ((danger_preds_binary == 1) & (danger_targets_binary == 1)).sum()
        false_positives = ((danger_preds_binary == 1) & (danger_targets_binary == 0)).sum()
        false_negatives = ((danger_preds_binary == 0) & (danger_targets_binary == 1)).sum()
        
        danger_precision = (true_positives / (true_positives + false_positives)) * 100 if (true_positives + false_positives) > 0 else 0.0
        danger_recall = (true_positives / (true_positives + false_negatives)) * 100 if (true_positives + false_negatives) > 0 else 0.0
        danger_f1 = (2 * danger_precision * danger_recall / (danger_precision + danger_recall)) if (danger_precision + danger_recall) > 0 else 0.0
        
        # 3. ìœ„í—˜ë„ MSE
        danger_mse = np.mean((all_danger_preds - all_danger_targets) ** 2)
        
        # 4. ì‹œë„ˆì§€ MSE (ì‹œë„ˆì§€ ë¼ë²¨ì´ ìˆëŠ” ê²½ìš°ë§Œ, ìœ„í—˜ì´ ì•„ë‹Œ ê²½ìš°)
        synergy_labeled_mask = (all_synergy_targets > 0) & (all_danger_targets == 0)
        if synergy_labeled_mask.sum() > 0:
            synergy_mse = np.mean((all_synergy_preds[synergy_labeled_mask] - all_synergy_targets[synergy_labeled_mask]) ** 2)
        else:
            synergy_mse = 0.0
        
        # 5. ì‹œë„ˆì§€ ìƒê´€ê³„ìˆ˜ (ì‹œë„ˆì§€ ë¼ë²¨ì´ ìˆëŠ” ê²½ìš°ë§Œ, ìœ„í—˜ì´ ì•„ë‹Œ ê²½ìš°)
        if synergy_labeled_mask.sum() > 0:
            synergy_correlation = np.corrcoef(all_synergy_preds[synergy_labeled_mask], 
                                             all_synergy_targets[synergy_labeled_mask])[0, 1]
        else:
            synergy_correlation = 0.0
        
        results = {
            'danger_accuracy': danger_accuracy,
            'danger_precision': danger_precision,
            'danger_recall': danger_recall,
            'danger_f1': danger_f1,
            'danger_mse': danger_mse,
            'synergy_mse': synergy_mse,
            'synergy_correlation': synergy_correlation,
            'danger_predictions': all_danger_preds,
            'synergy_predictions': all_synergy_preds,
            'danger_targets': all_danger_targets,
            'synergy_targets': all_synergy_targets,
            'labeled_samples': len(all_danger_targets),
            'total_samples': len(test_data),
            'synergy_labeled_samples': int(synergy_labeled_mask.sum()) if synergy_labeled_mask.sum() > 0 else 0
        }
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\n{'='*80}")
        print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ (ë¼ë²¨ì´ ìˆëŠ” ë°ì´í„°ë§Œ)")
        print(f"{'='*80}")
        print(f"í‰ê°€ëœ ìƒ˜í”Œ ìˆ˜: {len(all_danger_targets)}ê°œ (ì „ì²´ {len(test_data)}ê°œ ì¤‘)")
        print(f"ìœ„í—˜ë„ ë¶„ë¥˜ ì •í™•ë„: {danger_accuracy:.2f}%")
        print(f"ìœ„í—˜ë„ Precision: {danger_precision:.2f}%")
        print(f"ìœ„í—˜ë„ Recall: {danger_recall:.2f}%")
        print(f"ìœ„í—˜ë„ F1 Score: {danger_f1:.2f}%")
        print(f"ìœ„í—˜ë„ MSE: {danger_mse:.4f}")
        if synergy_labeled_mask.sum() > 0:
            print(f"ì‹œë„ˆì§€ MSE: {synergy_mse:.4f} (ì‹œë„ˆì§€ ë¼ë²¨ {int(synergy_labeled_mask.sum())}ê°œ)")
            print(f"ì‹œë„ˆì§€ ìƒê´€ê³„ìˆ˜: {synergy_correlation:.4f}")
        else:
            print(f"ì‹œë„ˆì§€ í‰ê°€: ì‹œë„ˆì§€ ë¼ë²¨ì´ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print(f"{'='*80}")
        
        return results

