{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a8d6c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API ì´ ë°ì´í„° ê±´ìˆ˜ë¥¼ í™•ì¸í•©ë‹ˆë‹¤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "âŒ ì´ˆê¸° API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: 500 Server Error: Internal Server Error for url: http://apis.data.go.kr/1471000/CsmtcsIngdCpntInfoService01/getCsmtcsIngdCpntInfoService01?serviceKey=50hjvXuloV4qFNdrIUOglZZ6RGV7uq7pvpP0oxT%2BEV57bvEGnWfvqbjL939z%2Fyfj9ta%2FH2Cn382mGmHpm4wmcw%3D%3D&pageNo=1&numOfRows=1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# 1. ë°œê¸‰ë°›ì€ API í‚¤ë¥¼ ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš”\n",
    "service_key = \"50hjvXuloV4qFNdrIUOglZZ6RGV7uq7pvpP0oxT+EV57bvEGnWfvqbjL939z/yfj9ta/H2Cn382mGmHpm4wmcw==\"\n",
    "url = \"http://apis.data.go.kr/1471000/CsmtcsIngdCpntInfoService01/getCsmtcsIngdCpntInfoService01\"\n",
    "# í•œ ë²ˆì˜ í˜¸ì¶œë¡œ ìµœëŒ€í•œ ë§ì€ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ë„ë¡ ì„¤ì •\n",
    "ROWS_PER_PAGE = 1000 \n",
    "OUTPUT_FILENAME = \"korea_standard_ingredients_api.json\"\n",
    "\n",
    "def fetch_all_ingredients():\n",
    "    \"\"\"ê³µê³µ APIë¥¼ ë°˜ë³µ í˜¸ì¶œí•˜ì—¬ ì „ì²´ ì„±ë¶„ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê³  íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    all_ingredients = []\n",
    "    \n",
    "    # 1. totalCount (ì´ ë°ì´í„° ê±´ìˆ˜) í™•ì¸ ìš”ì²­\n",
    "    print(\"API ì´ ë°ì´í„° ê±´ìˆ˜ë¥¼ í™•ì¸í•©ë‹ˆë‹¤...\")\n",
    "    params = {\"serviceKey\": service_key, \"pageNo\": 1, \"numOfRows\": 1}\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        root = ET.fromstring(response.content)\n",
    "        total_count_tag = root.find('.//totalCount')\n",
    "        \n",
    "        if total_count_tag is None:\n",
    "            print(\"âŒ totalCount ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. API ì‘ë‹µ í˜•ì‹ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "            return\n",
    "            \n",
    "        total_count = int(total_count_tag.text)\n",
    "        total_pages = math.ceil(total_count / ROWS_PER_PAGE)\n",
    "        \n",
    "        print(f\"âœ… ì´ {total_count}ê°œì˜ ì„±ë¶„ ì •ë³´ê°€ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ {total_pages} í˜ì´ì§€ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì´ˆê¸° API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\", file=sys.stderr)\n",
    "        return\n",
    "\n",
    "    # 2. ì „ì²´ í˜ì´ì§€ë¥¼ ìˆœíšŒí•˜ë©° ë°ì´í„° ìˆ˜ì§‘\n",
    "    for page_no in range(1, total_pages + 1):\n",
    "        print(f\"â¡ï¸ ë°ì´í„° ìˆ˜ì§‘ ì¤‘: Page {page_no} / {total_pages}\")\n",
    "        \n",
    "        params = {\n",
    "            \"serviceKey\": service_key, \n",
    "            \"pageNo\": page_no, \n",
    "            \"numOfRows\": ROWS_PER_PAGE\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=20)\n",
    "            response.raise_for_status()\n",
    "            root = ET.fromstring(response.content)\n",
    "            \n",
    "            items = root.findall('.//item')\n",
    "            \n",
    "            for item in items:\n",
    "                # í•„ìš”í•œ í•„ë“œ ì¶”ì¶œ\n",
    "                ingr_kor_name = item.find('INGR_KOR_NAME').text if item.find('INGR_KOR_NAME') is not None else None\n",
    "                ingr_eng_name = item.find('INGR_ENG_NAME').text if item.find('INGR_ENG_NAME') is not None else None\n",
    "                cas_no = item.find('CAS_NO').text if item.find('CAS_NO') is not None else None\n",
    "                \n",
    "                # ìœ íš¨í•œ í•œê¸€ ì„±ë¶„ëª…ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì €ì¥\n",
    "                if ingr_kor_name:\n",
    "                    all_ingredients.append({\n",
    "                        \"ì›ë£Œëª…\": ingr_kor_name,\n",
    "                        \"ì˜ë¬¸ëª…\": ingr_eng_name,\n",
    "                        \"CAS_NO\": cas_no\n",
    "                    })\n",
    "                    \n",
    "            # API í˜¸ì¶œ ê°„ ë¶€í•˜ ë°©ì§€ ë° ì•ˆì •ì„± í™•ë³´ë¥¼ ìœ„í•´ ì ì‹œ ëŒ€ê¸°\n",
    "            time.sleep(0.5) \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Page {page_no} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\", file=sys.stderr)\n",
    "            time.sleep(5) # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë” ì˜¤ë˜ ëŒ€ê¸° í›„ ì¬ì‹œë„ (í˜¹ì€ ë‹¤ìŒ í˜ì´ì§€ë¡œ)\n",
    "            continue\n",
    "            \n",
    "    # 3. ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "    if all_ingredients:\n",
    "        with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_ingredients, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(f\"\\nğŸ‰ ì „ì²´ ì„±ë¶„ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥ ì™„ë£Œ!\")\n",
    "        print(f\"ğŸ“¦ ì´ {len(all_ingredients)}ê°œì˜ ì„±ë¶„ì´ '{OUTPUT_FILENAME}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        # ì„±ë¶„ ì •ì œì— ì‚¬ìš©í•  í‘œì¤€ í•œê¸€ ì„±ë¶„ëª… ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\n",
    "        return [item['ì›ë£Œëª…'] for item in all_ingredients]\n",
    "    else:\n",
    "        print(\"âŒ ìˆ˜ì§‘ëœ ì„±ë¶„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return []\n",
    "\n",
    "# í•¨ìˆ˜ ì‹¤í–‰ ë° í‘œì¤€ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸ ì–»ê¸°\n",
    "standard_ingredients_list = fetch_all_ingredients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85438a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì›¹ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘...\n",
      "==================================================\n",
      "ğŸ” 1ë‹¨ê³„: ì´ 3 í˜ì´ì§€ë¥¼ íƒìƒ‰í•˜ì—¬ ì„±ë¶„ URLì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤...\n",
      "==================================================\n",
      "   íƒìƒ‰ ì¤‘: Page 1 / 3 | í˜„ì¬ URL ìˆ˜: 20ê°œ\n",
      "   íƒìƒ‰ ì¤‘: Page 3 / 3 | í˜„ì¬ URL ìˆ˜: 60ê°œ\n",
      "âœ… 1ë‹¨ê³„ ì™„ë£Œ: ì´ 60ê°œì˜ ì„±ë¶„ ìƒì„¸ URL ìˆ˜ì§‘ ì™„ë£Œ.\n",
      "\n",
      "==================================================\n",
      "ğŸ”¬ 2ë‹¨ê³„: ìˆ˜ì§‘ëœ 60ê°œì˜ URLì—ì„œ ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤...\n",
      "==================================================\n",
      "   ì§„í–‰ ìƒí™©: 5 / 60 (8.3%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 10 / 60 (16.7%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 15 / 60 (25.0%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 20 / 60 (33.3%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 25 / 60 (41.7%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 30 / 60 (50.0%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 35 / 60 (58.3%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 40 / 60 (66.7%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 45 / 60 (75.0%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 50 / 60 (83.3%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 55 / 60 (91.7%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 60 / 60 (100.0%) ì¶”ì¶œ ì™„ë£Œ\n",
      "\n",
      "==================================================\n",
      "ğŸ‰ ì‘ì—… ì™„ë£Œ!\n",
      "ì´ ì†Œìš” ì‹œê°„: 100.56ì´ˆ\n",
      "ìµœì¢… ë°ì´í„° íŒŒì¼: coos_ingredient_database.csv\n",
      "ì´ 60ê°œ ì„±ë¶„ ì €ì¥ ì™„ë£Œ.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, unquote\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1ë‹¨ê³„: ëª¨ë“  ìƒì„¸ URL ìˆ˜ì§‘ í•¨ìˆ˜\n",
    "# ----------------------------------------------------------------------\n",
    "def scrape_all_ingredient_urls(driver, max_page=1288):\n",
    "    \"\"\"\n",
    "    í˜ì´ì§€ë„¤ì´ì…˜ì„ ì´ìš©í•˜ì—¬ ëª¨ë“  ì„±ë¶„ ìƒì„¸ í˜ì´ì§€ì˜ URLì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    BASE_URL = \"https://coos.kr\"\n",
    "    all_detail_urls = set()\n",
    "    \n",
    "    # âš ï¸ ì‹¤ì œ ì „ì²´ ìŠ¤í¬ë˜í•‘ ì‹œì—ëŠ” max_page=1288ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì‘ì€ ê°’ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "    # max_page = 3  # í…ŒìŠ¤íŠ¸ìš©\n",
    "    \n",
    "    print(f\"==================================================\")\n",
    "    print(f\"ğŸ” 1ë‹¨ê³„: ì´ {max_page} í˜ì´ì§€ë¥¼ íƒìƒ‰í•˜ì—¬ ì„±ë¶„ URLì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤...\")\n",
    "    print(f\"==================================================\")\n",
    "    \n",
    "    for page_num in range(1, max_page + 1):\n",
    "        list_url = f\"{BASE_URL}/ingredients?page={page_num}\"\n",
    "        \n",
    "        try:\n",
    "            driver.get(list_url)\n",
    "            time.sleep(1.5) # ë¡œë”© ëŒ€ê¸° ì‹œê°„ (í™˜ê²½ì— ë”°ë¼ ì¡°ì • í•„ìš”)\n",
    "            \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            \n",
    "            # ì„±ë¶„ ìƒì„¸ URLì€ '/ingredients/'ë¡œ ì‹œì‘í•˜ê³  Query String(?)ì´ ì—†ëŠ” <a> íƒœê·¸ì˜ hrefë¥¼ ì´ìš©í•©ë‹ˆë‹¤.\n",
    "            link_tags = soup.find_all(\n",
    "                'a', \n",
    "                href=lambda href: href and href.startswith('/ingredients/') and '?' not in href\n",
    "            )\n",
    "            \n",
    "            # ìˆ˜ì§‘ ë° ì¤‘ë³µ ì œê±°\n",
    "            for tag in link_tags:\n",
    "                url_path = tag['href']\n",
    "                full_url = urljoin(BASE_URL, url_path)\n",
    "                all_detail_urls.add(full_url)\n",
    "            \n",
    "            if page_num % 100 == 0 or page_num == 1 or page_num == max_page:\n",
    "                print(f\"   íƒìƒ‰ ì¤‘: Page {page_num} / {max_page} | í˜„ì¬ URL ìˆ˜: {len(all_detail_urls)}ê°œ\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Page {page_num} ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({e}). ê±´ë„ˆë›°ê³  ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤.\")\n",
    "            continue\n",
    "            \n",
    "    print(f\"âœ… 1ë‹¨ê³„ ì™„ë£Œ: ì´ {len(all_detail_urls)}ê°œì˜ ì„±ë¶„ ìƒì„¸ URL ìˆ˜ì§‘ ì™„ë£Œ.\")\n",
    "    return list(all_detail_urls)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2ë‹¨ê³„: ìƒì„¸ í˜ì´ì§€ ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜\n",
    "# ----------------------------------------------------------------------\n",
    "def scrape_ingredient_details(driver, ingredient_url):\n",
    "    \"\"\"\n",
    "    ì„±ë¶„ ìƒì„¸ í˜ì´ì§€ì—ì„œ í•œê¸€ëª…, ì˜ë¬¸ëª…, CAS No.ë¥¼ ì•ˆì •ì ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    (ë™ì  CSS í´ë˜ìŠ¤ ëŒ€ì‹ , íƒœê·¸ì™€ í…ìŠ¤íŠ¸ ë ˆì´ë¸” ê¸°ë°˜)\n",
    "    \"\"\"\n",
    "    driver.get(ingredient_url)\n",
    "    time.sleep(1) # ë¡œë”© ëŒ€ê¸°\n",
    "    \n",
    "    details = {\n",
    "        'ì›ë£Œëª…': unquote(ingredient_url.split('/')[-1]), # URLì—ì„œ ì„ì‹œë¡œ ì´ë¦„ ì¶”ì¶œ\n",
    "        'ì˜ë¬¸ëª…': None,\n",
    "        'CAS_No': None,\n",
    "        'ì„¤ëª…_ìš”ì•½': None,\n",
    "        'íƒœê·¸_ëª©ë¡': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # 1. ì›ë£Œëª… (í•œê¸€) ì¶”ì¶œ: <h1> íƒœê·¸ ì‚¬ìš©\n",
    "        h1_tag = soup.find('h1')\n",
    "        if h1_tag:\n",
    "            details['ì›ë£Œëª…'] = h1_tag.text.strip()\n",
    "            \n",
    "            # 2. ì˜ë¬¸ëª… ì¶”ì¶œ: <h1> íƒœê·¸ ë‚´ë¶€ì— ì‘ì€ divë¡œ ë¶„ë¦¬ë˜ì–´ ìˆëŠ” ê²½ìš°\n",
    "            # <div style=\"...;\">...</div> í˜•íƒœë¥¼ ì°¾ìŠµë‹ˆë‹¤.\n",
    "            english_name_div = h1_tag.find_next_sibling('div', style=lambda s: s and 'font-size' in s)\n",
    "            if english_name_div:\n",
    "                 details['ì˜ë¬¸ëª…'] = english_name_div.text.strip()\n",
    "                 \n",
    "        # 3. CAS No. ë° ìƒì„¸ ì •ë³´ ì¶”ì¶œ: <dt> (ë ˆì´ë¸”) / <dd> (ê°’) ìŒ ê²€ìƒ‰\n",
    "        info_items = soup.find_all(['dt', 'dd'])\n",
    "        for i in range(len(info_items) - 1):\n",
    "            if info_items[i].name == 'dt':\n",
    "                label = info_items[i].text.strip()\n",
    "                value_tag = info_items[i+1]\n",
    "                value = value_tag.text.strip() if value_tag and value_tag.name == 'dd' else None\n",
    "                \n",
    "                if value and value.lower() != 'none':\n",
    "                    if 'INCI Name' in label:\n",
    "                        # ì´ë¯¸ 1ë²ˆì—ì„œ ì¶”ì¶œí–ˆìœ¼ë‚˜, ë‹¤ì‹œ í™•ì¸í•˜ëŠ” ìš©ë„\n",
    "                        if not details['ì˜ë¬¸ëª…']:\n",
    "                            details['ì˜ë¬¸ëª…'] = value\n",
    "                    elif 'CAS No' in label:\n",
    "                        details['CAS_No'] = value\n",
    "\n",
    "        # 4. ì„¤ëª… ìš”ì•½ ì¶”ì¶œ: í˜ì´ì§€ ë‚´ íŠ¹ì • CSS í´ë˜ìŠ¤ë¥¼ ê°€ì§„ divì—ì„œ ì¶”ì¶œ (ì´ì „ ë¶„ì„ ì°¸ê³ )\n",
    "        # í´ë˜ìŠ¤ê°€ ë³€ê²½ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë‚´ìš©ì´ ê¸´ ì²« ë²ˆì§¸ ë‹¨ë½ì„ ì°¾ëŠ” ê²ƒë„ ëŒ€ì•ˆ\n",
    "        summary_div = soup.find('div', class_=lambda c: c and 'css-m3enaf' in c)\n",
    "        if summary_div:\n",
    "            details['ì„¤ëª…_ìš”ì•½'] = summary_div.text.strip()\n",
    "            \n",
    "        # 5. íƒœê·¸/ì¹© ëª©ë¡ ì¶”ì¶œ: ëª¨ë“  ì¹© ë ˆì´ë¸” í…ìŠ¤íŠ¸ ìˆ˜ì§‘\n",
    "        chip_labels = soup.find_all('span', class_=lambda c: c and 'MuiChip-label' in c)\n",
    "        details['íƒœê·¸_ëª©ë¡'] = [label.text.strip() for label in chip_labels if label.text.strip()]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ì˜¤ë¥˜ ë°œìƒ: {ingredient_url} ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "    return details\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ë©”ì¸ ì‹¤í–‰ ë¸”ë¡\n",
    "# ----------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # --- ì„¤ì • ---\n",
    "    # âš ï¸ (í•„ìˆ˜) max_page ê°’ì„ '1288'ë¡œ ì„¤ì •í•´ì•¼ ëª¨ë“  í˜ì´ì§€ë¥¼ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤.\n",
    "    MAX_PAGE = 3 # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ì„ ìœ„í•´ 3í˜ì´ì§€ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "    OUTPUT_FILENAME = \"coos_ingredient_database.csv\"\n",
    "    \n",
    "    # 1. ì›¹ ë“œë¼ì´ë²„ ì„¤ì • (Selenium Managerê°€ ìë™ìœ¼ë¡œ ë“œë¼ì´ë²„ë¥¼ ì°¾ì•„ ì‚¬ìš©í•©ë‹ˆë‹¤.)\n",
    "    print(\"ğŸš€ ì›¹ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘...\")\n",
    "    try:\n",
    "        driver = webdriver.Chrome()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨. Chrome ì„¤ì¹˜ ë° Selenium ë²„ì „ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤: {e}\")\n",
    "        exit()\n",
    "        \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 2. 1ë‹¨ê³„ ì‹¤í–‰: ëª¨ë“  ìƒì„¸ URL ìˆ˜ì§‘\n",
    "    all_urls = scrape_all_ingredient_urls(driver, max_page=MAX_PAGE)\n",
    "    \n",
    "    # 3. 2ë‹¨ê³„ ì‹¤í–‰: ìƒì„¸ ì •ë³´ ì¶”ì¶œ\n",
    "    print(f\"\\n==================================================\")\n",
    "    print(f\"ğŸ”¬ 2ë‹¨ê³„: ìˆ˜ì§‘ëœ {len(all_urls)}ê°œì˜ URLì—ì„œ ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤...\")\n",
    "    print(f\"==================================================\")\n",
    "    \n",
    "    final_data = []\n",
    "    \n",
    "    for i, url in enumerate(all_urls):\n",
    "        data = scrape_ingredient_details(driver, url)\n",
    "        data['ìƒì„¸_URL'] = url\n",
    "        final_data.append(data)\n",
    "        \n",
    "        if (i + 1) % 5 == 0 or (i + 1) == len(all_urls):\n",
    "            print(f\"   ì§„í–‰ ìƒí™©: {i + 1} / {len(all_urls)} ({((i + 1) / len(all_urls) * 100):.1f}%) ì¶”ì¶œ ì™„ë£Œ\")\n",
    "\n",
    "    # 4. ë“œë¼ì´ë²„ ì¢…ë£Œ\n",
    "    driver.quit()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # 5. ë°ì´í„° ì €ì¥ ë° ì •ë¦¬\n",
    "    df = pd.DataFrame(final_data)\n",
    "    \n",
    "    # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ íƒœê·¸ ëª©ë¡ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ CSVì— ì €ì¥í•˜ê¸° ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤.\n",
    "    df['íƒœê·¸_ëª©ë¡'] = df['íƒœê·¸_ëª©ë¡'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "    \n",
    "    df.to_csv(OUTPUT_FILENAME, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\n==================================================\")\n",
    "    print(f\"ğŸ‰ ì‘ì—… ì™„ë£Œ!\")\n",
    "    print(f\"ì´ ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ\")\n",
    "    print(f\"ìµœì¢… ë°ì´í„° íŒŒì¼: {OUTPUT_FILENAME}\")\n",
    "    print(f\"ì´ {len(df)}ê°œ ì„±ë¶„ ì €ì¥ ì™„ë£Œ.\")\n",
    "    print(f\"==================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afbed366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì›¹ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘...\n",
      "==================================================\n",
      "ğŸ” 1ë‹¨ê³„: ì´ 1288 í˜ì´ì§€ë¥¼ íƒìƒ‰í•˜ì—¬ ì„±ë¶„ URLì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤...\n",
      "==================================================\n",
      "   íƒìƒ‰ ì¤‘: Page 1 / 1288 | í˜„ì¬ URL ìˆ˜: 20ê°œ\n",
      "   íƒìƒ‰ ì¤‘: Page 100 / 1288 | í˜„ì¬ URL ìˆ˜: 2000ê°œ\n",
      "   íƒìƒ‰ ì¤‘: Page 200 / 1288 | í˜„ì¬ URL ìˆ˜: 3998ê°œ\n",
      "   íƒìƒ‰ ì¤‘: Page 300 / 1288 | í˜„ì¬ URL ìˆ˜: 5998ê°œ\n",
      "   íƒìƒ‰ ì¤‘: Page 400 / 1288 | í˜„ì¬ URL ìˆ˜: 7998ê°œ\n",
      "   íƒìƒ‰ ì¤‘: Page 500 / 1288 | í˜„ì¬ URL ìˆ˜: 9998ê°œ\n",
      "   íƒìƒ‰ ì¤‘: Page 600 / 1288 | í˜„ì¬ URL ìˆ˜: 11998ê°œ\n",
      "   íƒìƒ‰ ì¤‘: Page 700 / 1288 | í˜„ì¬ URL ìˆ˜: 13998ê°œ\n",
      "   íƒìƒ‰ ì¤‘: Page 800 / 1288 | í˜„ì¬ URL ìˆ˜: 15998ê°œ\n",
      "   íƒìƒ‰ ì¤‘: Page 900 / 1288 | í˜„ì¬ URL ìˆ˜: 17998ê°œ\n",
      "   íƒìƒ‰ ì¤‘: Page 1000 / 1288 | í˜„ì¬ URL ìˆ˜: 19998ê°œ\n",
      "   íƒìƒ‰ ì¤‘: Page 1100 / 1288 | í˜„ì¬ URL ìˆ˜: 21998ê°œ\n",
      "   íƒìƒ‰ ì¤‘: Page 1200 / 1288 | í˜„ì¬ URL ìˆ˜: 23998ê°œ\n",
      "   íƒìƒ‰ ì¤‘: Page 1288 / 1288 | í˜„ì¬ URL ìˆ˜: 25750ê°œ\n",
      "âœ… 1ë‹¨ê³„ ì™„ë£Œ: ì´ 25750ê°œì˜ ì„±ë¶„ ìƒì„¸ URL ìˆ˜ì§‘ ì™„ë£Œ.\n",
      "\n",
      "==================================================\n",
      "ğŸ”¬ 2ë‹¨ê³„: ìˆ˜ì§‘ëœ 25750ê°œì˜ URLì—ì„œ ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤...\n",
      "==================================================\n",
      "   ì§„í–‰ ìƒí™©: 1 / 25750 (0.0%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 50 / 25750 (0.2%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 100 / 25750 (0.4%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 150 / 25750 (0.6%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 200 / 25750 (0.8%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 250 / 25750 (1.0%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 300 / 25750 (1.2%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 350 / 25750 (1.4%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 400 / 25750 (1.6%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 450 / 25750 (1.7%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 500 / 25750 (1.9%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 550 / 25750 (2.1%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 600 / 25750 (2.3%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 650 / 25750 (2.5%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 700 / 25750 (2.7%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 750 / 25750 (2.9%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 800 / 25750 (3.1%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 850 / 25750 (3.3%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 900 / 25750 (3.5%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 950 / 25750 (3.7%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 1000 / 25750 (3.9%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 1050 / 25750 (4.1%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 1100 / 25750 (4.3%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 1150 / 25750 (4.5%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 1200 / 25750 (4.7%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 1250 / 25750 (4.9%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 1300 / 25750 (5.0%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 1350 / 25750 (5.2%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 1400 / 25750 (5.4%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 1450 / 25750 (5.6%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 1500 / 25750 (5.8%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 1550 / 25750 (6.0%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 1600 / 25750 (6.2%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 1650 / 25750 (6.4%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 1700 / 25750 (6.6%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 1750 / 25750 (6.8%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 1800 / 25750 (7.0%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 1850 / 25750 (7.2%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 1900 / 25750 (7.4%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 1950 / 25750 (7.6%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 2000 / 25750 (7.8%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 2050 / 25750 (8.0%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 2100 / 25750 (8.2%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 2150 / 25750 (8.3%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 2200 / 25750 (8.5%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 2250 / 25750 (8.7%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 2300 / 25750 (8.9%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 2350 / 25750 (9.1%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 2400 / 25750 (9.3%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 2450 / 25750 (9.5%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 2500 / 25750 (9.7%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 2550 / 25750 (9.9%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 2600 / 25750 (10.1%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 2650 / 25750 (10.3%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 2700 / 25750 (10.5%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 2750 / 25750 (10.7%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 2800 / 25750 (10.9%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 2850 / 25750 (11.1%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 2900 / 25750 (11.3%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 2950 / 25750 (11.5%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 3000 / 25750 (11.7%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 3050 / 25750 (11.8%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 3100 / 25750 (12.0%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 3150 / 25750 (12.2%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 3200 / 25750 (12.4%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 3250 / 25750 (12.6%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 3300 / 25750 (12.8%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 3350 / 25750 (13.0%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 3400 / 25750 (13.2%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 3450 / 25750 (13.4%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 3500 / 25750 (13.6%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 3550 / 25750 (13.8%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 3600 / 25750 (14.0%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 3650 / 25750 (14.2%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 3700 / 25750 (14.4%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 3750 / 25750 (14.6%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 3800 / 25750 (14.8%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 3850 / 25750 (15.0%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 3900 / 25750 (15.1%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 3950 / 25750 (15.3%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 4000 / 25750 (15.5%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 4050 / 25750 (15.7%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 4100 / 25750 (15.9%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 4150 / 25750 (16.1%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 4200 / 25750 (16.3%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 4250 / 25750 (16.5%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 4300 / 25750 (16.7%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 4350 / 25750 (16.9%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 4400 / 25750 (17.1%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 4450 / 25750 (17.3%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 4500 / 25750 (17.5%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 4550 / 25750 (17.7%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 4600 / 25750 (17.9%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 4650 / 25750 (18.1%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 4700 / 25750 (18.3%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 4750 / 25750 (18.4%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 4800 / 25750 (18.6%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 4850 / 25750 (18.8%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 4900 / 25750 (19.0%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 4950 / 25750 (19.2%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 5000 / 25750 (19.4%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 5050 / 25750 (19.6%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 5100 / 25750 (19.8%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 5150 / 25750 (20.0%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 5200 / 25750 (20.2%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 5250 / 25750 (20.4%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 5300 / 25750 (20.6%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 5350 / 25750 (20.8%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 5400 / 25750 (21.0%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 5450 / 25750 (21.2%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 5500 / 25750 (21.4%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 5550 / 25750 (21.6%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 5600 / 25750 (21.7%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 5650 / 25750 (21.9%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 5700 / 25750 (22.1%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 5750 / 25750 (22.3%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 5800 / 25750 (22.5%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 5850 / 25750 (22.7%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 5900 / 25750 (22.9%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 5950 / 25750 (23.1%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 6000 / 25750 (23.3%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 6050 / 25750 (23.5%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 6100 / 25750 (23.7%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 6150 / 25750 (23.9%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 6200 / 25750 (24.1%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 6250 / 25750 (24.3%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 6300 / 25750 (24.5%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 6350 / 25750 (24.7%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 6400 / 25750 (24.9%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 6450 / 25750 (25.0%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 6500 / 25750 (25.2%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 6550 / 25750 (25.4%) ì¶”ì¶œ ì™„ë£Œ\n",
      "   ì§„í–‰ ìƒí™©: 6600 / 25750 (25.6%) ì¶”ì¶œ ì™„ë£Œ\n"
     ]
    },
    {
     "ename": "ReadTimeoutError",
     "evalue": "HTTPConnectionPool(host='localhost', port=64907): Read timed out. (read timeout=120)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/Project_A/.conda/lib/python3.11/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/Project_A/.conda/lib/python3.11/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/Project_A/.conda/lib/python3.11/http/client.py:1395\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1394\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1395\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/Project_A/.conda/lib/python3.11/http/client.py:325\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/Project_A/.conda/lib/python3.11/http/client.py:286\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/Project_A/.conda/lib/python3.11/socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[31mTimeoutError\u001b[39m: timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mReadTimeoutError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 162\u001b[39m\n\u001b[32m    159\u001b[39m final_data = []\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, url \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_urls):\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     data = \u001b[43mscrape_ingredient_details\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m     data[\u001b[33m'\u001b[39m\u001b[33mìƒì„¸_URL\u001b[39m\u001b[33m'\u001b[39m] = url\n\u001b[32m    164\u001b[39m     final_data.append(data)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mscrape_ingredient_details\u001b[39m\u001b[34m(driver, ingredient_url)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscrape_ingredient_details\u001b[39m(driver, ingredient_url):\n\u001b[32m     60\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[33;03m    ì„±ë¶„ ìƒì„¸ í˜ì´ì§€ì—ì„œ í•œê¸€ëª…, ì˜ë¬¸ëª…, CAS No. ë° íƒœê·¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[33;03m    (íƒœê·¸ ëª©ë¡ì—ì„œ ë¶ˆí•„ìš”í•œ ë…¸ì´ì¦ˆ ì½”ë“œ ì œê±° ë¡œì§ í¬í•¨)\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mingredient_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     time.sleep(\u001b[32m1\u001b[39m) \u001b[38;5;66;03m# ë¡œë”© ëŒ€ê¸°\u001b[39;00m\n\u001b[32m     67\u001b[39m     details = {\n\u001b[32m     68\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mì›ë£Œëª…\u001b[39m\u001b[33m'\u001b[39m: unquote(ingredient_url.split(\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m)[-\u001b[32m1\u001b[39m]), \n\u001b[32m     69\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mì˜ë¬¸ëª…\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     72\u001b[39m         \u001b[33m'\u001b[39m\u001b[33míƒœê·¸_ëª©ë¡\u001b[39m\u001b[33m'\u001b[39m: []\n\u001b[32m     73\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/Project_A/.conda/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:483\u001b[39m, in \u001b[36mWebDriver.get\u001b[39m\u001b[34m(self, url)\u001b[39m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    466\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Navigate the browser to the specified URL in the current window or\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[33;03m    tab.\u001b[39;00m\n\u001b[32m    468\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    481\u001b[39m \u001b[33;03m    >>> driver.get(\"https://example.com\")\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43murl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/Project_A/.conda/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:455\u001b[39m, in \u001b[36mWebDriver.execute\u001b[39m\u001b[34m(self, driver_command, params)\u001b[39m\n\u001b[32m    452\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msessionId\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[32m    453\u001b[39m         params[\u001b[33m\"\u001b[39m\u001b[33msessionId\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.session_id\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m response = \u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRemoteConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcommand_executor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28mself\u001b[39m.error_handler.check_response(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/Project_A/.conda/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py:406\u001b[39m, in \u001b[36mRemoteConnection.execute\u001b[39m\u001b[34m(self, command, params)\u001b[39m\n\u001b[32m    404\u001b[39m trimmed = \u001b[38;5;28mself\u001b[39m._trim_large_entries(params)\n\u001b[32m    405\u001b[39m LOGGER.debug(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, command_info[\u001b[32m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_info\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/Project_A/.conda/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py:430\u001b[39m, in \u001b[36mRemoteConnection._request\u001b[39m\u001b[34m(self, method, url, body)\u001b[39m\n\u001b[32m    427\u001b[39m     body = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client_config.keep_alive:\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    431\u001b[39m     statuscode = response.status\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/Project_A/.conda/lib/python3.11/site-packages/urllib3/_request_methods.py:143\u001b[39m, in \u001b[36mRequestMethods.request\u001b[39m\u001b[34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[39m\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request_encode_url(\n\u001b[32m    136\u001b[39m         method,\n\u001b[32m    137\u001b[39m         url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m         **urlopen_kw,\n\u001b[32m    141\u001b[39m     )\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest_encode_body\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43murlopen_kw\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/Project_A/.conda/lib/python3.11/site-packages/urllib3/_request_methods.py:278\u001b[39m, in \u001b[36mRequestMethods.request_encode_body\u001b[39m\u001b[34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[39m\n\u001b[32m    274\u001b[39m     extra_kw[\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m].setdefault(\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m, content_type)\n\u001b[32m    276\u001b[39m extra_kw.update(urlopen_kw)\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/Project_A/.conda/lib/python3.11/site-packages/urllib3/poolmanager.py:459\u001b[39m, in \u001b[36mPoolManager.urlopen\u001b[39m\u001b[34m(self, method, url, redirect, **kw)\u001b[39m\n\u001b[32m    457\u001b[39m     response = conn.urlopen(method, url, **kw)\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    461\u001b[39m redirect_location = redirect \u001b[38;5;129;01mand\u001b[39;00m response.get_redirect_location()\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/Project_A/.conda/lib/python3.11/site-packages/urllib3/connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    838\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_e, (\u001b[38;5;167;01mOSError\u001b[39;00m, HTTPException)):\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n\u001b[32m    846\u001b[39m \u001b[38;5;66;03m# Keep track of the error for the retry warning.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/Project_A/.conda/lib/python3.11/site-packages/urllib3/util/retry.py:474\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    471\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_read_error(error):\n\u001b[32m    472\u001b[39m     \u001b[38;5;66;03m# Read retry?\u001b[39;00m\n\u001b[32m    473\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_method_retryable(method):\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    476\u001b[39m         read -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/Project_A/.conda/lib/python3.11/site-packages/urllib3/util/util.py:39\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(tp, value, tb)\u001b[39m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m value.__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m value.with_traceback(tb)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     41\u001b[39m     value = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/Project_A/.conda/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/Project_A/.conda/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    534\u001b[39m     response = conn.getresponse()\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    537\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    539\u001b[39m \u001b[38;5;66;03m# Set properties that are used by the pooling layer.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/Project_A/.conda/lib/python3.11/site-packages/urllib3/connectionpool.py:367\u001b[39m, in \u001b[36mHTTPConnectionPool._raise_timeout\u001b[39m\u001b[34m(self, err, url, timeout_value)\u001b[39m\n\u001b[32m    364\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Is the error actually a timeout? Will raise a ReadTimeout or pass\"\"\"\u001b[39;00m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[32m    368\u001b[39m         \u001b[38;5;28mself\u001b[39m, url, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    369\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(err, \u001b[33m\"\u001b[39m\u001b[33merrno\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m err.errno \u001b[38;5;129;01min\u001b[39;00m _blocking_errnos:\n",
      "\u001b[31mReadTimeoutError\u001b[39m: HTTPConnectionPool(host='localhost', port=64907): Read timed out. (read timeout=120)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, unquote\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1ë‹¨ê³„: ëª¨ë“  ìƒì„¸ URL ìˆ˜ì§‘ í•¨ìˆ˜\n",
    "# ----------------------------------------------------------------------\n",
    "def scrape_all_ingredient_urls(driver, max_page):\n",
    "    \"\"\"\n",
    "    í˜ì´ì§€ë„¤ì´ì…˜ì„ ì´ìš©í•˜ì—¬ ëª¨ë“  ì„±ë¶„ ìƒì„¸ í˜ì´ì§€ì˜ URLì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    BASE_URL = \"https://coos.kr\"\n",
    "    all_detail_urls = set()\n",
    "    \n",
    "    print(f\"==================================================\")\n",
    "    print(f\"ğŸ” 1ë‹¨ê³„: ì´ {max_page} í˜ì´ì§€ë¥¼ íƒìƒ‰í•˜ì—¬ ì„±ë¶„ URLì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤...\")\n",
    "    print(f\"==================================================\")\n",
    "    \n",
    "    for page_num in range(1, max_page + 1):\n",
    "        list_url = f\"{BASE_URL}/ingredients?page={page_num}\"\n",
    "        \n",
    "        try:\n",
    "            driver.get(list_url)\n",
    "            # ë¡œë”© ëŒ€ê¸° ì‹œê°„: í™˜ê²½ì— ë”°ë¼ ì¡°ì •ì´ í•„ìš”í•˜ë©°, ë„ˆë¬´ ì§§ìœ¼ë©´ ë°ì´í„° ëˆ„ë½, ë„ˆë¬´ ê¸¸ë©´ ì‹œê°„ ë‚­ë¹„\n",
    "            time.sleep(1.5) \n",
    "            \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            \n",
    "            # ì„±ë¶„ ìƒì„¸ URLì€ '/ingredients/'ë¡œ ì‹œì‘í•˜ê³  Query String(?)ì´ ì—†ëŠ” <a> íƒœê·¸ì˜ hrefë¥¼ ì´ìš©\n",
    "            link_tags = soup.find_all(\n",
    "                'a', \n",
    "                href=lambda href: href and href.startswith('/ingredients/') and '?' not in href\n",
    "            )\n",
    "            \n",
    "            # ìˆ˜ì§‘ ë° ì¤‘ë³µ ì œê±°\n",
    "            for tag in link_tags:\n",
    "                url_path = tag['href']\n",
    "                full_url = urljoin(BASE_URL, url_path)\n",
    "                all_detail_urls.add(full_url)\n",
    "            \n",
    "            if page_num % 100 == 0 or page_num == 1 or page_num == max_page:\n",
    "                print(f\"   íƒìƒ‰ ì¤‘: Page {page_num} / {max_page} | í˜„ì¬ URL ìˆ˜: {len(all_detail_urls)}ê°œ\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Page {page_num} ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({e}). ê±´ë„ˆë›°ê³  ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤.\")\n",
    "            # ì˜¤ë¥˜ê°€ ë‚˜ë„ í”„ë¡œê·¸ë¨ì„ ë©ˆì¶”ì§€ ì•Šê³  ë‹¤ìŒ í˜ì´ì§€ë¡œ ê³„ì† ì§„í–‰\n",
    "            continue\n",
    "            \n",
    "    print(f\"âœ… 1ë‹¨ê³„ ì™„ë£Œ: ì´ {len(all_detail_urls)}ê°œì˜ ì„±ë¶„ ìƒì„¸ URL ìˆ˜ì§‘ ì™„ë£Œ.\")\n",
    "    return list(all_detail_urls)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2ë‹¨ê³„: ìƒì„¸ í˜ì´ì§€ ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜ (íƒœê·¸ ë…¸ì´ì¦ˆ í•„í„°ë§ ì ìš©)\n",
    "# ----------------------------------------------------------------------\n",
    "def scrape_ingredient_details(driver, ingredient_url):\n",
    "    \"\"\"\n",
    "    ì„±ë¶„ ìƒì„¸ í˜ì´ì§€ì—ì„œ í•œê¸€ëª…, ì˜ë¬¸ëª…, CAS No. ë° íƒœê·¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    (íƒœê·¸ ëª©ë¡ì—ì„œ ë¶ˆí•„ìš”í•œ ë…¸ì´ì¦ˆ ì½”ë“œ ì œê±° ë¡œì§ í¬í•¨)\n",
    "    \"\"\"\n",
    "    driver.get(ingredient_url)\n",
    "    time.sleep(1) # ë¡œë”© ëŒ€ê¸°\n",
    "    \n",
    "    details = {\n",
    "        'ì›ë£Œëª…': unquote(ingredient_url.split('/')[-1]), \n",
    "        'ì˜ë¬¸ëª…': None,\n",
    "        'CAS_No': None,\n",
    "        'ì„¤ëª…_ìš”ì•½': None,\n",
    "        'íƒœê·¸_ëª©ë¡': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # 1. ì›ë£Œëª… (í•œê¸€) ì¶”ì¶œ: <h1> íƒœê·¸\n",
    "        h1_tag = soup.find('h1')\n",
    "        if h1_tag:\n",
    "            details['ì›ë£Œëª…'] = h1_tag.text.strip()\n",
    "            \n",
    "            # 2. ì˜ë¬¸ëª… ì¶”ì¶œ: <h1> ë°”ë¡œ ë‹¤ìŒì˜ div\n",
    "            english_name_div = h1_tag.find_next_sibling('div', style=lambda s: s and 'font-size' in s)\n",
    "            if english_name_div:\n",
    "                 details['ì˜ë¬¸ëª…'] = english_name_div.text.strip()\n",
    "                 \n",
    "        # 3. CAS No. ë° ìƒì„¸ ì •ë³´ ì¶”ì¶œ: <dt> (ë ˆì´ë¸”) / <dd> (ê°’) ìŒ ê²€ìƒ‰\n",
    "        info_items = soup.find_all(['dt', 'dd'])\n",
    "        for i in range(len(info_items) - 1):\n",
    "            if info_items[i].name == 'dt':\n",
    "                label = info_items[i].text.strip()\n",
    "                value_tag = info_items[i+1]\n",
    "                value = value_tag.text.strip() if value_tag and value_tag.name == 'dd' else None\n",
    "                \n",
    "                if value and value.lower() != 'none':\n",
    "                    if 'INCI Name' in label and not details['ì˜ë¬¸ëª…']:\n",
    "                        details['ì˜ë¬¸ëª…'] = value\n",
    "                    elif 'CAS No' in label:\n",
    "                        details['CAS_No'] = value\n",
    "\n",
    "        # 4. ì„¤ëª… ìš”ì•½ ì¶”ì¶œ: CSS í´ë˜ìŠ¤ ê¸°ë°˜\n",
    "        summary_div = soup.find('div', class_=lambda c: c and 'css-m3enaf' in c)\n",
    "        if summary_div:\n",
    "            details['ì„¤ëª…_ìš”ì•½'] = summary_div.text.strip()\n",
    "            \n",
    "        # 5. íƒœê·¸/ì¹© ëª©ë¡ ì¶”ì¶œ ë° í•„í„°ë§\n",
    "        chip_labels = soup.find_all('span', class_=lambda c: c and 'MuiChip-label' in c)\n",
    "        \n",
    "        # ğŸš« í•„í„°ë§í•  ë¶ˆí•„ìš”í•œ ë…¸ì´ì¦ˆ íƒœê·¸ ëª©ë¡ ì •ì˜\n",
    "        # ì§§ì€ ê¸¸ì´, êµ­ê°€/ì–¸ì–´ ì½”ë“œ, ë¶ˆëª…í™•í•œ ìƒíƒœ ì½”ë“œ ì œê±°\n",
    "        NOISE_TAGS = {\n",
    "            'KO', 'EN', 'JP', 'AI', 'EU', 'ì‹', 'ê°€ëŠ¥', 'ë¶ˆê°€', 'AI,', 'EU,', \n",
    "            'ì‹,', 'ê°€ëŠ¥,', 'ë¶ˆê°€,', 'INCI Name', 'CAS No' # ë ˆì´ë¸”ì´ íƒœê·¸ë¡œ ì˜ëª» ë“¤ì–´ì˜¤ëŠ” ê²½ìš° ë°©ì§€\n",
    "        } \n",
    "        \n",
    "        extracted_tags = []\n",
    "        for label in chip_labels:\n",
    "            tag_text = label.text.strip()\n",
    "            # í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì§€ ì•Šê³ , ë…¸ì´ì¦ˆ ëª©ë¡ì— ì—†ìœ¼ë©°, ê¸¸ì´ê°€ 2ì ì´ˆê³¼ì¸ ê²½ìš°ë§Œ ì¶”ì¶œ\n",
    "            if tag_text and tag_text not in NOISE_TAGS and len(tag_text) > 2: \n",
    "                extracted_tags.append(tag_text)\n",
    "                \n",
    "        details['íƒœê·¸_ëª©ë¡'] = extracted_tags\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ì˜¤ë¥˜ ë°œìƒ: {ingredient_url} ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "    return details\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ë©”ì¸ ì‹¤í–‰ ë¸”ë¡\n",
    "# ----------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # --- ì„¤ì • ---\n",
    "    # âš ï¸ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•œ ìµœì¢… ì„¤ì •ì…ë‹ˆë‹¤.\n",
    "    MAX_PAGE = 1288 \n",
    "    OUTPUT_FILENAME = \"coos_ingredient_database_final.csv\"\n",
    "    \n",
    "    # 1. ì›¹ ë“œë¼ì´ë²„ ì„¤ì • (Selenium Managerê°€ ìë™ìœ¼ë¡œ ë“œë¼ì´ë²„ë¥¼ ì°¾ì•„ ì‚¬ìš©í•©ë‹ˆë‹¤.)\n",
    "    print(\"ğŸš€ ì›¹ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘...\")\n",
    "    try:\n",
    "        driver = webdriver.Chrome()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨. Chrome ì„¤ì¹˜ ë° Selenium ë²„ì „ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤: {e}\")\n",
    "        exit()\n",
    "        \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 2. 1ë‹¨ê³„ ì‹¤í–‰: ëª¨ë“  ìƒì„¸ URL ìˆ˜ì§‘\n",
    "    # max_pageë¥¼ ì¸ìë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "    all_urls = scrape_all_ingredient_urls(driver, max_page=MAX_PAGE)\n",
    "    \n",
    "    # 3. 2ë‹¨ê³„ ì‹¤í–‰: ìƒì„¸ ì •ë³´ ì¶”ì¶œ\n",
    "    print(f\"\\n==================================================\")\n",
    "    print(f\"ğŸ”¬ 2ë‹¨ê³„: ìˆ˜ì§‘ëœ {len(all_urls)}ê°œì˜ URLì—ì„œ ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤...\")\n",
    "    print(f\"==================================================\")\n",
    "    \n",
    "    final_data = []\n",
    "    \n",
    "    for i, url in enumerate(all_urls):\n",
    "        data = scrape_ingredient_details(driver, url)\n",
    "        data['ìƒì„¸_URL'] = url\n",
    "        final_data.append(data)\n",
    "        \n",
    "        # ì§„í–‰ ìƒí™© ì¶œë ¥\n",
    "        if (i + 1) % 50 == 0 or (i + 1) == len(all_urls) or i == 0:\n",
    "            print(f\"   ì§„í–‰ ìƒí™©: {i + 1} / {len(all_urls)} ({((i + 1) / len(all_urls) * 100):.1f}%) ì¶”ì¶œ ì™„ë£Œ\")\n",
    "\n",
    "    # 4. ë“œë¼ì´ë²„ ì¢…ë£Œ\n",
    "    driver.quit()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # 5. ë°ì´í„° ì €ì¥ ë° ì •ë¦¬\n",
    "    df = pd.DataFrame(final_data)\n",
    "    \n",
    "    # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ íƒœê·¸ ëª©ë¡ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ CSVì— ì €ì¥\n",
    "    df['íƒœê·¸_ëª©ë¡'] = df['íƒœê·¸_ëª©ë¡'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "    \n",
    "    df.to_csv(OUTPUT_FILENAME, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\n==================================================\")\n",
    "    print(f\"ğŸ‰ ì‘ì—… ì™„ë£Œ!\")\n",
    "    print(f\"ì´ ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ\")\n",
    "    print(f\"ìµœì¢… ë°ì´í„° íŒŒì¼: {OUTPUT_FILENAME}\")\n",
    "    print(f\"ì´ {len(df)}ê°œ ì„±ë¶„ ì €ì¥ ì™„ë£Œ.\")\n",
    "    print(f\"==================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7edaa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì›¹ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘...\n",
      "   -> ê¸°ì¡´ URL íŒŒì¼ì—ì„œ 300ê°œì˜ URLì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n",
      "   âœ… 1ë‹¨ê³„ ì¬ê°œ: 16 í˜ì´ì§€ë¶€í„° íƒìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "==================================================\n",
      "ğŸ” 1ë‹¨ê³„: ì´ 1288 í˜ì´ì§€ ì¤‘ 16 í˜ì´ì§€ë¶€í„° íƒìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n",
      "==================================================\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 50 / 1288 | í˜„ì¬ URL ìˆ˜: 1000ê°œ\n",
      "   === 100í˜ì´ì§€ íœ´ì‹ ì‹œì‘ (10ì´ˆ) ===\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 100 / 1288 | í˜„ì¬ URL ìˆ˜: 2000ê°œ\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 150 / 1288 | í˜„ì¬ URL ìˆ˜: 3000ê°œ\n",
      "   === 100í˜ì´ì§€ íœ´ì‹ ì‹œì‘ (10ì´ˆ) ===\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 200 / 1288 | í˜„ì¬ URL ìˆ˜: 3998ê°œ\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 250 / 1288 | í˜„ì¬ URL ìˆ˜: 4998ê°œ\n",
      "   === 100í˜ì´ì§€ íœ´ì‹ ì‹œì‘ (10ì´ˆ) ===\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 300 / 1288 | í˜„ì¬ URL ìˆ˜: 5998ê°œ\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 350 / 1288 | í˜„ì¬ URL ìˆ˜: 6998ê°œ\n",
      "   === 100í˜ì´ì§€ íœ´ì‹ ì‹œì‘ (10ì´ˆ) ===\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 400 / 1288 | í˜„ì¬ URL ìˆ˜: 7998ê°œ\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 450 / 1288 | í˜„ì¬ URL ìˆ˜: 8998ê°œ\n",
      "   === 100í˜ì´ì§€ íœ´ì‹ ì‹œì‘ (10ì´ˆ) ===\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 500 / 1288 | í˜„ì¬ URL ìˆ˜: 9998ê°œ\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 550 / 1288 | í˜„ì¬ URL ìˆ˜: 10998ê°œ\n",
      "   === 100í˜ì´ì§€ íœ´ì‹ ì‹œì‘ (10ì´ˆ) ===\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 600 / 1288 | í˜„ì¬ URL ìˆ˜: 11998ê°œ\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 650 / 1288 | í˜„ì¬ URL ìˆ˜: 12998ê°œ\n",
      "   === 100í˜ì´ì§€ íœ´ì‹ ì‹œì‘ (10ì´ˆ) ===\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 700 / 1288 | í˜„ì¬ URL ìˆ˜: 13998ê°œ\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 750 / 1288 | í˜„ì¬ URL ìˆ˜: 14998ê°œ\n",
      "   === 100í˜ì´ì§€ íœ´ì‹ ì‹œì‘ (10ì´ˆ) ===\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 800 / 1288 | í˜„ì¬ URL ìˆ˜: 15998ê°œ\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 850 / 1288 | í˜„ì¬ URL ìˆ˜: 16998ê°œ\n",
      "   === 100í˜ì´ì§€ íœ´ì‹ ì‹œì‘ (10ì´ˆ) ===\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 900 / 1288 | í˜„ì¬ URL ìˆ˜: 17998ê°œ\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 950 / 1288 | í˜„ì¬ URL ìˆ˜: 18998ê°œ\n",
      "   === 100í˜ì´ì§€ íœ´ì‹ ì‹œì‘ (10ì´ˆ) ===\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 1000 / 1288 | í˜„ì¬ URL ìˆ˜: 19998ê°œ\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 1050 / 1288 | í˜„ì¬ URL ìˆ˜: 20998ê°œ\n",
      "   === 100í˜ì´ì§€ íœ´ì‹ ì‹œì‘ (10ì´ˆ) ===\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 1100 / 1288 | í˜„ì¬ URL ìˆ˜: 21998ê°œ\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 1150 / 1288 | í˜„ì¬ URL ìˆ˜: 22998ê°œ\n",
      "   === 100í˜ì´ì§€ íœ´ì‹ ì‹œì‘ (10ì´ˆ) ===\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 1200 / 1288 | í˜„ì¬ URL ìˆ˜: 23998ê°œ\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 1250 / 1288 | í˜„ì¬ URL ìˆ˜: 24998ê°œ\n",
      "   íƒìƒ‰ ì™„ë£Œ: Page 1288 / 1288 | í˜„ì¬ URL ìˆ˜: 25750ê°œ\n",
      "âœ… 1ë‹¨ê³„ ì™„ë£Œ: ì´ 25750ê°œì˜ ì„±ë¶„ ìƒì„¸ URL ìˆ˜ì§‘ ì™„ë£Œ.\n",
      "\n",
      "âœ… ê¸°ì¡´ ìƒì„¸ ë°ì´í„° íŒŒì¼ 'coos_ingredient_database.csv' ë°œê²¬. 2ë‹¨ê³„ ì¬ê°œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.\n",
      "   -> ì´ë¯¸ 60ê°œì˜ ì„±ë¶„ ë°ì´í„°ê°€ ìˆ˜ì§‘ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë¯¸ìˆ˜ì§‘ URLë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
      "\n",
      "==================================================\n",
      "ğŸ”¬ 2ë‹¨ê³„: ë¯¸ì²˜ë¦¬ëœ 25690ê°œì˜ URLì—ì„œ ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤...\n",
      "==================================================\n",
      "   ì§„í–‰ ìƒí™©: 1 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 62ê°œ (0.2%)\n",
      "   ì§„í–‰ ìƒí™©: 50 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 160ê°œ (0.6%)\n",
      "   ì§„í–‰ ìƒí™©: 100 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 260ê°œ (1.0%)\n",
      "   ì§„í–‰ ìƒí™©: 150 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 360ê°œ (1.4%)\n",
      "   ì§„í–‰ ìƒí™©: 200 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 460ê°œ (1.8%)\n",
      "   ì§„í–‰ ìƒí™©: 250 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 560ê°œ (2.2%)\n",
      "   ì§„í–‰ ìƒí™©: 300 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 660ê°œ (2.6%)\n",
      "   ì§„í–‰ ìƒí™©: 350 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 760ê°œ (3.0%)\n",
      "   ì§„í–‰ ìƒí™©: 400 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 860ê°œ (3.3%)\n",
      "   ì§„í–‰ ìƒí™©: 450 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 960ê°œ (3.7%)\n",
      "   ì§„í–‰ ìƒí™©: 500 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 1060ê°œ (4.1%)\n",
      "   ì§„í–‰ ìƒí™©: 550 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 1160ê°œ (4.5%)\n",
      "   ì§„í–‰ ìƒí™©: 600 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 1260ê°œ (4.9%)\n",
      "   ì§„í–‰ ìƒí™©: 650 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 1360ê°œ (5.3%)\n",
      "   ì§„í–‰ ìƒí™©: 700 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 1460ê°œ (5.7%)\n",
      "   ì§„í–‰ ìƒí™©: 750 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 1560ê°œ (6.1%)\n",
      "   ì§„í–‰ ìƒí™©: 800 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 1660ê°œ (6.4%)\n",
      "   ì§„í–‰ ìƒí™©: 850 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 1760ê°œ (6.8%)\n",
      "   ì§„í–‰ ìƒí™©: 900 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 1860ê°œ (7.2%)\n",
      "   ì§„í–‰ ìƒí™©: 950 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 1960ê°œ (7.6%)\n",
      "   ì§„í–‰ ìƒí™©: 1000 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 2060ê°œ (8.0%)\n",
      "   ì§„í–‰ ìƒí™©: 1050 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 2160ê°œ (8.4%)\n",
      "   ì§„í–‰ ìƒí™©: 1100 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 2260ê°œ (8.8%)\n",
      "   ì§„í–‰ ìƒí™©: 1150 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 2360ê°œ (9.2%)\n",
      "   ì§„í–‰ ìƒí™©: 1200 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 2460ê°œ (9.6%)\n",
      "   ì§„í–‰ ìƒí™©: 1250 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 2560ê°œ (9.9%)\n",
      "   ì§„í–‰ ìƒí™©: 1300 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 2660ê°œ (10.3%)\n",
      "   ì§„í–‰ ìƒí™©: 1350 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 2760ê°œ (10.7%)\n",
      "   ì§„í–‰ ìƒí™©: 1400 / 25690 ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ 2860ê°œ (11.1%)\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: unknown error: net::ERR_INTERNET_DISCONNECTED\n  (Session info: chrome=141.0.7390.66)\nStacktrace:\n0   chromedriver                        0x000000010087f5f0 cxxbridge1$str$ptr + 2894960\n1   chromedriver                        0x000000010087752c cxxbridge1$str$ptr + 2861996\n2   chromedriver                        0x000000010039d5ec _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74324\n3   chromedriver                        0x00000001003956b0 _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 41752\n4   chromedriver                        0x0000000100388220 cxxbridge1$string$len + 4708\n5   chromedriver                        0x0000000100389cb8 cxxbridge1$string$len + 11516\n6   chromedriver                        0x0000000100388698 cxxbridge1$string$len + 5852\n7   chromedriver                        0x0000000100387f88 cxxbridge1$string$len + 4044\n8   chromedriver                        0x0000000100387d14 cxxbridge1$string$len + 3416\n9   chromedriver                        0x0000000100385b1c chromedriver + 203548\n10  chromedriver                        0x0000000100386574 chromedriver + 206196\n11  chromedriver                        0x00000001003a0660 _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 86728\n12  chromedriver                        0x0000000100426f10 _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 637816\n13  chromedriver                        0x0000000100426458 _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 635072\n14  chromedriver                        0x00000001003d9178 _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318944\n15  chromedriver                        0x000000010084333c cxxbridge1$str$ptr + 2648508\n16  chromedriver                        0x0000000100846918 cxxbridge1$str$ptr + 2662296\n17  chromedriver                        0x0000000100823dbc cxxbridge1$str$ptr + 2520124\n18  chromedriver                        0x0000000100847200 cxxbridge1$str$ptr + 2664576\n19  chromedriver                        0x000000010081552c cxxbridge1$str$ptr + 2460588\n20  chromedriver                        0x0000000100866bec cxxbridge1$str$ptr + 2794092\n21  chromedriver                        0x0000000100866d70 cxxbridge1$str$ptr + 2794480\n22  chromedriver                        0x0000000100877178 cxxbridge1$str$ptr + 2861048\n23  libsystem_pthread.dylib             0x0000000188a33c0c _pthread_start + 136\n24  libsystem_pthread.dylib             0x0000000188a2eb80 thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mWebDriverException\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 225\u001b[39m\n\u001b[32m    221\u001b[39m final_data = existing_data \u001b[38;5;66;03m# ê¸°ì¡´ ë°ì´í„° + ìƒˆë¡œ ìˆ˜ì§‘í•  ë°ì´í„°\u001b[39;00m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, url \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pending_urls):\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m     data = \u001b[43mscrape_ingredient_details\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    226\u001b[39m     data[\u001b[33m'\u001b[39m\u001b[33mìƒì„¸_URL\u001b[39m\u001b[33m'\u001b[39m] = url\n\u001b[32m    227\u001b[39m     final_data.append(data)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 103\u001b[39m, in \u001b[36mscrape_ingredient_details\u001b[39m\u001b[34m(driver, ingredient_url)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscrape_ingredient_details\u001b[39m(driver, ingredient_url):\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mingredient_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m     \u001b[38;5;66;03m# ğŸŒŸ ëŒ€ê¸° ì‹œê°„ ê°•í™”: 1ì´ˆ ~ 2.5ì´ˆ ì‚¬ì´ì˜ ë¬´ì‘ìœ„ ëŒ€ê¸°\u001b[39;00m\n\u001b[32m    105\u001b[39m     time.sleep(random.uniform(\u001b[32m1.0\u001b[39m, \u001b[32m2.5\u001b[39m)) \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/Project_A/.conda/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:483\u001b[39m, in \u001b[36mWebDriver.get\u001b[39m\u001b[34m(self, url)\u001b[39m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    466\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Navigate the browser to the specified URL in the current window or\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[33;03m    tab.\u001b[39;00m\n\u001b[32m    468\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    481\u001b[39m \u001b[33;03m    >>> driver.get(\"https://example.com\")\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43murl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/Project_A/.conda/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:458\u001b[39m, in \u001b[36mWebDriver.execute\u001b[39m\u001b[34m(self, driver_command, params)\u001b[39m\n\u001b[32m    455\u001b[39m response = cast(RemoteConnection, \u001b[38;5;28mself\u001b[39m.command_executor).execute(driver_command, params)\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merror_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m     response[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._unwrap_value(response.get(\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    460\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/Project_A/.conda/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py:232\u001b[39m, in \u001b[36mErrorHandler.check_response\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    230\u001b[39m         alert_text = value[\u001b[33m\"\u001b[39m\u001b[33malert\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[31mWebDriverException\u001b[39m: Message: unknown error: net::ERR_INTERNET_DISCONNECTED\n  (Session info: chrome=141.0.7390.66)\nStacktrace:\n0   chromedriver                        0x000000010087f5f0 cxxbridge1$str$ptr + 2894960\n1   chromedriver                        0x000000010087752c cxxbridge1$str$ptr + 2861996\n2   chromedriver                        0x000000010039d5ec _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74324\n3   chromedriver                        0x00000001003956b0 _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 41752\n4   chromedriver                        0x0000000100388220 cxxbridge1$string$len + 4708\n5   chromedriver                        0x0000000100389cb8 cxxbridge1$string$len + 11516\n6   chromedriver                        0x0000000100388698 cxxbridge1$string$len + 5852\n7   chromedriver                        0x0000000100387f88 cxxbridge1$string$len + 4044\n8   chromedriver                        0x0000000100387d14 cxxbridge1$string$len + 3416\n9   chromedriver                        0x0000000100385b1c chromedriver + 203548\n10  chromedriver                        0x0000000100386574 chromedriver + 206196\n11  chromedriver                        0x00000001003a0660 _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 86728\n12  chromedriver                        0x0000000100426f10 _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 637816\n13  chromedriver                        0x0000000100426458 _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 635072\n14  chromedriver                        0x00000001003d9178 _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318944\n15  chromedriver                        0x000000010084333c cxxbridge1$str$ptr + 2648508\n16  chromedriver                        0x0000000100846918 cxxbridge1$str$ptr + 2662296\n17  chromedriver                        0x0000000100823dbc cxxbridge1$str$ptr + 2520124\n18  chromedriver                        0x0000000100847200 cxxbridge1$str$ptr + 2664576\n19  chromedriver                        0x000000010081552c cxxbridge1$str$ptr + 2460588\n20  chromedriver                        0x0000000100866bec cxxbridge1$str$ptr + 2794092\n21  chromedriver                        0x0000000100866d70 cxxbridge1$str$ptr + 2794480\n22  chromedriver                        0x0000000100877178 cxxbridge1$str$ptr + 2861048\n23  libsystem_pthread.dylib             0x0000000188a33c0c _pthread_start + 136\n24  libsystem_pthread.dylib             0x0000000188a2eb80 thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import random # ğŸŒŸ ë¬´ì‘ìœ„ ëŒ€ê¸° ì‹œê°„ ì ìš©ì„ ìœ„í•´ ì¶”ê°€\n",
    "import os # ğŸŒŸ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ì„ ìœ„í•´ ì¶”ê°€\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, unquote\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1ë‹¨ê³„: ëª¨ë“  ìƒì„¸ URL ìˆ˜ì§‘ í•¨ìˆ˜ (ì§€ì—° ì‹œê°„ ê°•í™” ë° í˜ì´ì§€ ì¬ê°œ ê¸°ëŠ¥ í¬í•¨)\n",
    "# ----------------------------------------------------------------------\n",
    "def scrape_all_ingredient_urls(driver, max_page, url_file, last_page_file):\n",
    "    \"\"\"\n",
    "    í˜ì´ì§€ë„¤ì´ì…˜ì„ ì´ìš©í•˜ì—¬ ëª¨ë“  ì„±ë¶„ ìƒì„¸ í˜ì´ì§€ì˜ URLì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    BASE_URL = \"https://coos.kr\"\n",
    "    all_detail_urls = set()\n",
    "    \n",
    "    # ğŸŒŸ 1-1. ê¸°ì¡´ URL íŒŒì¼ì—ì„œ URL ëª©ë¡ ë¡œë“œ (ë§Œì•½ 1ë‹¨ê³„ê°€ ì™„ì „íˆ ì™„ë£Œë˜ì§€ ì•Šì•˜ì„ ê²½ìš°)\n",
    "    if os.path.exists(url_file):\n",
    "        try:\n",
    "            with open(url_file, 'r', encoding='utf-8') as f:\n",
    "                loaded_urls = {line.strip() for line in f if line.strip()}\n",
    "            all_detail_urls.update(loaded_urls)\n",
    "            print(f\"   -> ê¸°ì¡´ URL íŒŒì¼ì—ì„œ {len(all_detail_urls)}ê°œì˜ URLì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "        except:\n",
    "             print(\"   -> ê¸°ì¡´ URL íŒŒì¼ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° URLì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    # ğŸŒŸ 1-2. ë§ˆì§€ë§‰ ì„±ê³µ í˜ì´ì§€ ë²ˆí˜¸ ë¡œë“œ\n",
    "    start_page = 1\n",
    "    if os.path.exists(last_page_file):\n",
    "        try:\n",
    "            with open(last_page_file, 'r') as f:\n",
    "                start_page = int(f.read().strip())\n",
    "                # ì´ë¯¸ ì²˜ë¦¬ëœ í˜ì´ì§€ëŠ” ê±´ë„ˆëœë‹ˆë‹¤.\n",
    "                print(f\"   âœ… 1ë‹¨ê³„ ì¬ê°œ: {start_page} í˜ì´ì§€ë¶€í„° íƒìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "        except:\n",
    "            pass # íŒŒì¼ì´ ì†ìƒë˜ì—ˆë‹¤ë©´ 1ë¶€í„° ì‹œì‘\n",
    "\n",
    "    print(f\"==================================================\")\n",
    "    print(f\"ğŸ” 1ë‹¨ê³„: ì´ {max_page} í˜ì´ì§€ ì¤‘ {start_page} í˜ì´ì§€ë¶€í„° íƒìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "    print(f\"==================================================\")\n",
    "    \n",
    "    for page_num in range(start_page, max_page + 1):\n",
    "        list_url = f\"{BASE_URL}/ingredients?page={page_num}\"\n",
    "        \n",
    "        try:\n",
    "            driver.get(list_url)\n",
    "            # ğŸŒŸ ëŒ€ê¸° ì‹œê°„ ê°•í™”: 1.5ì´ˆ ~ 3.5ì´ˆ ì‚¬ì´ì˜ ë¬´ì‘ìœ„ ëŒ€ê¸°\n",
    "            time.sleep(random.uniform(1.5, 3.5)) \n",
    "            \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            \n",
    "            link_tags = soup.find_all(\n",
    "                'a', \n",
    "                href=lambda href: href and href.startswith('/ingredients/') and '?' not in href\n",
    "            )\n",
    "            \n",
    "            # ìˆ˜ì§‘ ë° ì¤‘ë³µ ì œê±°\n",
    "            new_urls = []\n",
    "            for tag in link_tags:\n",
    "                url_path = tag['href']\n",
    "                full_url = urljoin(BASE_URL, url_path)\n",
    "                if full_url not in all_detail_urls:\n",
    "                    all_detail_urls.add(full_url)\n",
    "                    new_urls.append(full_url)\n",
    "            \n",
    "            # ğŸŒŸ í˜ì´ì§€ ì„±ê³µ ì‹œ í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸ ì €ì¥ ë° ìˆ˜ì§‘ëœ URL ëª©ë¡ ì¶”ê°€ ì €ì¥\n",
    "            with open(last_page_file, 'w') as f:\n",
    "                f.write(str(page_num + 1)) # ë‹¤ìŒ ì‹œì‘ í˜ì´ì§€ë¥¼ ì €ì¥\n",
    "\n",
    "            with open(url_file, 'a', encoding='utf-8') as f:\n",
    "                for url in new_urls:\n",
    "                    f.write(f\"{url}\\n\")\n",
    "            \n",
    "            # ğŸŒŸ ë§¤ 100í˜ì´ì§€ë§ˆë‹¤ ë” ê¸´ íœ´ì‹ ì‹œê°„ì„ ê°€ì§‘ë‹ˆë‹¤. (ì„œë²„ ë¶€í•˜ ê°ì†Œ)\n",
    "            if page_num % 100 == 0:\n",
    "                print(f\"   === 100í˜ì´ì§€ íœ´ì‹ ì‹œì‘ (10ì´ˆ) ===\")\n",
    "                time.sleep(10)\n",
    "                \n",
    "            if page_num % 50 == 0 or page_num == max_page:\n",
    "                print(f\"   íƒìƒ‰ ì™„ë£Œ: Page {page_num} / {max_page} | í˜„ì¬ URL ìˆ˜: {len(all_detail_urls)}ê°œ\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Page {page_num} ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({e}). {last_page_file}ì— ì €ì¥ í›„ ì¢…ë£Œ ëŒ€ê¸°...\")\n",
    "            time.sleep(5) # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¶”ê°€ ëŒ€ê¸° í›„ ì¬ì‹œë„\n",
    "            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë§ˆì§€ë§‰ ì„±ê³µ í˜ì´ì§€ê°€ ì €ì¥ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, í”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë˜ì–´ë„ ì¬ê°œ ê°€ëŠ¥\n",
    "            raise # ì˜¤ë¥˜ ë°œìƒ ì‹œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¤‘ë‹¨í•˜ì—¬ ì¬ì‹¤í–‰ì„ ìœ ë„\n",
    "\n",
    "    # ğŸŒŸ ëª¨ë“  í˜ì´ì§€ íƒìƒ‰ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ë©´ í˜ì´ì§€ ì¶”ì  íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤.\n",
    "    if os.path.exists(last_page_file):\n",
    "        os.remove(last_page_file)\n",
    "            \n",
    "    print(f\"âœ… 1ë‹¨ê³„ ì™„ë£Œ: ì´ {len(all_detail_urls)}ê°œì˜ ì„±ë¶„ ìƒì„¸ URL ìˆ˜ì§‘ ì™„ë£Œ.\")\n",
    "    return list(all_detail_urls)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2ë‹¨ê³„: ìƒì„¸ í˜ì´ì§€ ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜ (íƒœê·¸ ë…¸ì´ì¦ˆ í•„í„°ë§ ì ìš©)\n",
    "# ----------------------------------------------------------------------\n",
    "def scrape_ingredient_details(driver, ingredient_url):\n",
    "    driver.get(ingredient_url)\n",
    "    # ğŸŒŸ ëŒ€ê¸° ì‹œê°„ ê°•í™”: 1ì´ˆ ~ 2.5ì´ˆ ì‚¬ì´ì˜ ë¬´ì‘ìœ„ ëŒ€ê¸°\n",
    "    time.sleep(random.uniform(1.0, 2.5)) \n",
    "    \n",
    "    details = {\n",
    "        'ì›ë£Œëª…': unquote(ingredient_url.split('/')[-1]), \n",
    "        'ì˜ë¬¸ëª…': None,\n",
    "        'CAS_No': None,\n",
    "        'ì„¤ëª…_ìš”ì•½': None,\n",
    "        'íƒœê·¸_ëª©ë¡': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # 1. ì›ë£Œëª… (í•œê¸€) ì¶”ì¶œ: <h1> íƒœê·¸\n",
    "        h1_tag = soup.find('h1')\n",
    "        if h1_tag:\n",
    "            details['ì›ë£Œëª…'] = h1_tag.text.strip()\n",
    "            \n",
    "            # 2. ì˜ë¬¸ëª… ì¶”ì¶œ: <h1> ë°”ë¡œ ë‹¤ìŒì˜ div\n",
    "            english_name_div = h1_tag.find_next_sibling('div', style=lambda s: s and 'font-size' in s)\n",
    "            if english_name_div:\n",
    "                 details['ì˜ë¬¸ëª…'] = english_name_div.text.strip()\n",
    "                 \n",
    "        # 3. CAS No. ë° ìƒì„¸ ì •ë³´ ì¶”ì¶œ: <dt> (ë ˆì´ë¸”) / <dd> (ê°’) ìŒ ê²€ìƒ‰\n",
    "        info_items = soup.find_all(['dt', 'dd'])\n",
    "        for i in range(len(info_items) - 1):\n",
    "            if info_items[i].name == 'dt':\n",
    "                label = info_items[i].text.strip()\n",
    "                value_tag = info_items[i+1]\n",
    "                value = value_tag.text.strip() if value_tag and value_tag.name == 'dd' else None\n",
    "                \n",
    "                if value and value.lower() != 'none':\n",
    "                    if 'INCI Name' in label and not details['ì˜ë¬¸ëª…']:\n",
    "                        details['ì˜ë¬¸ëª…'] = value\n",
    "                    elif 'CAS No' in label:\n",
    "                        details['CAS_No'] = value\n",
    "\n",
    "        # 4. ì„¤ëª… ìš”ì•½ ì¶”ì¶œ: CSS í´ë˜ìŠ¤ ê¸°ë°˜\n",
    "        summary_div = soup.find('div', class_=lambda c: c and 'css-m3enaf' in c)\n",
    "        if summary_div:\n",
    "            details['ì„¤ëª…_ìš”ì•½'] = summary_div.text.strip()\n",
    "            \n",
    "        # 5. íƒœê·¸/ì¹© ëª©ë¡ ì¶”ì¶œ ë° í•„í„°ë§\n",
    "        chip_labels = soup.find_all('span', class_=lambda c: c and 'MuiChip-label' in c)\n",
    "        NOISE_TAGS = {\n",
    "            'KO', 'EN', 'JP', 'AI', 'EU', 'ì‹', 'ê°€ëŠ¥', 'ë¶ˆê°€', 'AI,', 'EU,', \n",
    "            'ì‹,', 'ê°€ëŠ¥,', 'ë¶ˆê°€,', 'INCI Name', 'CAS No' \n",
    "        } \n",
    "        \n",
    "        extracted_tags = []\n",
    "        for label in chip_labels:\n",
    "            tag_text = label.text.strip()\n",
    "            if tag_text and tag_text not in NOISE_TAGS and len(tag_text) > 2: \n",
    "                extracted_tags.append(tag_text)\n",
    "                \n",
    "        details['íƒœê·¸_ëª©ë¡'] = extracted_tags\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ì˜¤ë¥˜ ë°œìƒ: {ingredient_url} ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "        time.sleep(3) # ìƒì„¸ í˜ì´ì§€ ì˜¤ë¥˜ ì‹œ ì¶”ê°€ ëŒ€ê¸°\n",
    "        \n",
    "    return details\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ë©”ì¸ ì‹¤í–‰ ë¸”ë¡\n",
    "# ----------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # --- ì„¤ì • ---\n",
    "    MAX_PAGE = 1288 \n",
    "    OUTPUT_FILENAME = \"coos_ingredient_database.csv\"\n",
    "    # ğŸŒŸ 1ë‹¨ê³„ URL ìˆ˜ì§‘ ì¬ê°œë¥¼ ìœ„í•œ íŒŒì¼ ì¶”ê°€\n",
    "    URL_LIST_FILE = \"coos_all_urls.txt\"\n",
    "    LAST_PAGE_FILE = \"last_scraped_page.txt\"\n",
    "    \n",
    "    # 1. ì›¹ ë“œë¼ì´ë²„ ì„¤ì •\n",
    "    print(\"ğŸš€ ì›¹ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘...\")\n",
    "    try:\n",
    "        driver = webdriver.Chrome()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨. Chrome ì„¤ì¹˜ ë° Selenium ë²„ì „ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤: {e}\")\n",
    "        exit()\n",
    "        \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 2. 1ë‹¨ê³„ ì‹¤í–‰: ëª¨ë“  ìƒì„¸ URL ìˆ˜ì§‘ (í˜ì´ì§€ ì¬ê°œ ê¸°ëŠ¥ ì‚¬ìš©)\n",
    "    try:\n",
    "        all_urls = scrape_all_ingredient_urls(driver, max_page=MAX_PAGE, url_file=URL_LIST_FILE, last_page_file=LAST_PAGE_FILE)\n",
    "    except Exception as e:\n",
    "        # 1ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜ê°€ ë‚˜ë©´ ë“œë¼ì´ë²„ë§Œ ë‹«ê³  ì¢…ë£Œí•˜ì—¬ ì¬ì‹¤í–‰ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
    "        print(f\"\\nâŒ 1ë‹¨ê³„ URL ìˆ˜ì§‘ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}. ë‹¤ìŒ ì¬ì‹¤í–‰ ì‹œ {LAST_PAGE_FILE}ë¶€í„° ì¬ê°œë©ë‹ˆë‹¤.\")\n",
    "        driver.quit()\n",
    "        exit()\n",
    "    \n",
    "    # ğŸŒŸ 3. ì¬ê°œ ë¡œì§ ì ìš© ë° URL ëª©ë¡ í•„í„°ë§ (2ë‹¨ê³„)\n",
    "    scraped_urls = set()\n",
    "    existing_data = []\n",
    "\n",
    "    if os.path.exists(OUTPUT_FILENAME):\n",
    "        print(f\"\\nâœ… ê¸°ì¡´ ìƒì„¸ ë°ì´í„° íŒŒì¼ '{OUTPUT_FILENAME}' ë°œê²¬. 2ë‹¨ê³„ ì¬ê°œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.\")\n",
    "        try:\n",
    "            df_existing = pd.read_csv(OUTPUT_FILENAME, encoding='utf-8-sig')\n",
    "            scraped_urls = set(df_existing['ìƒì„¸_URL'].dropna())\n",
    "            existing_data = df_existing.to_dict('records')\n",
    "            \n",
    "            existing_data = [d for d in existing_data if d.get('ìƒì„¸_URL')] \n",
    "            print(f\"   -> ì´ë¯¸ {len(scraped_urls)}ê°œì˜ ì„±ë¶„ ë°ì´í„°ê°€ ìˆ˜ì§‘ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë¯¸ìˆ˜ì§‘ URLë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\")\n",
    "        except Exception as e:\n",
    "             print(f\"   -> ê¸°ì¡´ íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({e}). ì²˜ìŒë¶€í„° 2ë‹¨ê³„ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    # ë¯¸ìˆ˜ì§‘ëœ URLë§Œ ë‚¨ê¹ë‹ˆë‹¤.\n",
    "    pending_urls = [url for url in all_urls if url not in scraped_urls]\n",
    "    \n",
    "    print(f\"\\n==================================================\")\n",
    "    print(f\"ğŸ”¬ 2ë‹¨ê³„: ë¯¸ì²˜ë¦¬ëœ {len(pending_urls)}ê°œì˜ URLì—ì„œ ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤...\")\n",
    "    print(f\"==================================================\")\n",
    "    \n",
    "    final_data = existing_data # ê¸°ì¡´ ë°ì´í„° + ìƒˆë¡œ ìˆ˜ì§‘í•  ë°ì´í„°\n",
    "\n",
    "    for i, url in enumerate(pending_urls):\n",
    "        \n",
    "        data = scrape_ingredient_details(driver, url)\n",
    "        data['ìƒì„¸_URL'] = url\n",
    "        final_data.append(data)\n",
    "        \n",
    "        # ğŸŒŸ ë§¤ 100ê°œë§ˆë‹¤ ì¤‘ê°„ ì €ì¥ (ë°ì´í„° ìœ ì‹¤ ë°©ì§€)\n",
    "        if (len(final_data) - len(existing_data)) % 100 == 0 and (len(final_data) - len(existing_data)) > 0:\n",
    "            df_temp = pd.DataFrame(final_data)\n",
    "            df_temp['íƒœê·¸_ëª©ë¡'] = df_temp['íƒœê·¸_ëª©ë¡'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "            df_temp.to_csv(OUTPUT_FILENAME, index=False, encoding='utf-8-sig')\n",
    "            print(f\"   [ì¤‘ê°„ ì €ì¥] í˜„ì¬ê¹Œì§€ ì´ {len(final_data)}ê°œ ì„±ë¶„ ì €ì¥ ì™„ë£Œ.\")\n",
    "\n",
    "        current_processed = i + 1\n",
    "        total_pending = len(pending_urls)\n",
    "        \n",
    "        if current_processed % 50 == 0 or current_processed == total_pending or current_processed == 1:\n",
    "            total_scraped_now = len(existing_data) + current_processed\n",
    "            print(f\"   ì§„í–‰ ìƒí™©: {current_processed} / {total_pending} ë¯¸ì²˜ë¦¬ í•­ëª© ì™„ë£Œ | ì´ {total_scraped_now}ê°œ ({total_scraped_now / len(all_urls) * 100:.1f}%)\")\n",
    "\n",
    "\n",
    "    # 4. ë“œë¼ì´ë²„ ì¢…ë£Œ\n",
    "    driver.quit()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # 5. ìµœì¢… ë°ì´í„° ì €ì¥ ë° ì •ë¦¬\n",
    "    df = pd.DataFrame(final_data)\n",
    "    \n",
    "    # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ íƒœê·¸ ëª©ë¡ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ CSVì— ì €ì¥\n",
    "    df['íƒœê·¸_ëª©ë¡'] = df['íƒœê·¸_ëª©ë¡'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "    \n",
    "    df.to_csv(OUTPUT_FILENAME, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\n==================================================\")\n",
    "    print(f\"ğŸ‰ ì‘ì—… ì™„ë£Œ!\")\n",
    "    print(f\"ì´ ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ\")\n",
    "    print(f\"ìµœì¢… ë°ì´í„° íŒŒì¼: {OUTPUT_FILENAME}\")\n",
    "    print(f\"ì´ {len(df)}ê°œ ì„±ë¶„ ì €ì¥ ì™„ë£Œ.\")\n",
    "    print(f\"==================================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
